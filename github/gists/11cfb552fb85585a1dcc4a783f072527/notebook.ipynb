{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/ruvnet/11cfb552fb85585a1dcc4a783f072527/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6zoZA7_02ko"
      },
      "source": [
        "# Deep Codestral: Fine-Tuning Codestral 25.01 with Unsloth\n",
        "\n",
        "This notebook demonstrates how to fine-tune Mistral AI's **Codestral 25.01** (a state-of-the-art coding model that generates code approximately 2× faster than its predecessor) for enhanced reasoning—what we call **architect mode**. In this configuration (Deep Codestral), the model is fine-tuned to produce chain-of-thought, step-by-step explanations for system design and architectural planning.\n",
        "\n",
        "Created by rUv (because he could), Deep Codestral leverages LoRA (Low-Rank Adaptation) for efficient fine-tuning, with an optional GSPO (Generalized Structured Prompt Optimization) extension. GSPO refines prompt structures using agentic datasets in JSONL or Parquet formats.\n",
        "\n",
        "The notebook covers:\n",
        "1. Environment and GPU setup\n",
        "2. Library installation\n",
        "3. Preparing a reasoning dataset (with optional loading from Strawberry-Phi examples)\n",
        "4. Loading Codestral 25.01 in 4-bit mode\n",
        "5. Configuring LoRA and (optionally) GSPO\n",
        "6. Fine-tuning with an evaluation framework\n",
        "7. Saving and exporting the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9McK87Dh02lR"
      },
      "source": [
        "## 1. Environment Setup and GPU Verification\n",
        "\n",
        "Ensure that the notebook is running in GPU mode. Go to **Runtime > Change runtime type > Hardware Accelerator > GPU**. This cell checks Python version, CUDA availability, and lists available GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIxevZzb02le"
      },
      "source": [
        "!python --version\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVKwnppG02lx"
      },
      "source": [
        "## 2. Install Required Libraries\n",
        "\n",
        "We install the libraries needed for fine-tuning:\n",
        "- **Unsloth**: Optimized library for LLM fine-tuning (supports 4-bit quantization and LoRA)\n",
        "- **PEFT**: Parameter-Efficient Fine-Tuning for applying LoRA adapters\n",
        "- **bitsandbytes**: For 4-bit model loading\n",
        "- **TRL**: Provides `SFTTrainer` for supervised fine-tuning\n",
        "- **Transformers, Datasets, Accelerate**: Hugging Face core libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut_nTwRe02l1"
      },
      "source": [
        "%%bash\n",
        "pip install -q unsloth peft bitsandbytes trl transformers accelerate datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh4VA_-D02l4"
      },
      "source": [
        "!pip list | grep -E 'unsloth|peft|bitsandbytes|trl|transformers'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoNmsLog02l7"
      },
      "source": [
        "## 3. Prepare a Reasoning Dataset\n",
        "\n",
        "We create a small sample reasoning dataset in JSONL format. Optionally, you can load the agentic training examples from rUv's Strawberry-Phi repository. Two sources are provided:\n",
        "- Parquet: [strawberry-phi.parquet](https://github.com/ruvnet/strawberry-phi/blob/main/finetune/strawberry-phi.parquet)\n",
        "- JSONL: [strawberry-phi.jsonl](https://github.com/ruvnet/strawberry-phi/blob/main/finetune/strawberry-phi.jsonl)\n",
        "\n",
        "For demonstration, we create a small local JSONL file. To use the agentic examples, update the file path accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlEU7eB402mC"
      },
      "source": [
        "%%bash\n",
        "cat > reasoning_dataset.jsonl <<'DATA'\n",
        "{\"prompt\": \"Question: What is 12 * 7? Let's think step-by-step.\", \"completion\": \"First, break down the problem: 12 * 7 means adding 12 seven times. 10 * 7 = 70 and 2 * 7 = 14, so 70 + 14 = 84. Therefore, the answer is 84.\"}\n",
        "{\"prompt\": \"Question: If Alice is taller than Bob, and Bob is taller than Charlie, who is the tallest? Explain your reasoning.\", \"completion\": \"Given Alice > Bob and Bob > Charlie, Alice is taller than both. Therefore, Alice is the tallest.\"}\n",
        "{\"prompt\": \"Question: John has 3 more apples than Sam. Sam has 2 apples. How many apples does John have? Show your reasoning.\", \"completion\": \"Sam has 2 apples. John has 3 more, so 2 + 3 = 5 apples.\"}\n",
        "DATA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScYaS_iR02mP"
      },
      "source": [
        "## 4. Load Codestral 25.01 in 4-bit with Unsloth\n",
        "\n",
        "We load the Codestral 25.01 model (the instruct variant) in 4-bit mode using Unsloth. This reduces memory usage significantly while retaining model performance. Make sure you have sufficient VRAM (≈12–16GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xx5rgxP02mV"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model_name = \"unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit\"\n",
        "max_seq_length = 2048  # set maximum sequence length\n",
        "\n",
        "# Load model and tokenizer in 4-bit mode\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    load_in_4bit=True,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKFE89H102m1"
      },
      "source": [
        "## 5. Configure LoRA Fine-Tuning and Optional GSPO\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** is applied so that only a small fraction of parameters are fine-tuned. This drastically reduces memory and computational requirements.\n",
        "\n",
        "Optionally, we can integrate **GSPO (Generalized Structured Prompt Optimization)**. GSPO optimizes prompt segmentation and structure, using agentic datasets (JSONL or Parquet). To use GSPO, set the flag and load your agentic data file accordingly.\n",
        "\n",
        "In this example, we proceed with LoRA fine-tuning. To enable GSPO, one might load additional data and run an optimization routine (this is provided as an optional section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgiU1X7P02m4"
      },
      "source": [
        "# Attach LoRA adapters to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        "    max_seq_length=max_seq_length\n",
        ")\n",
        "\n",
        "# Optional: GSPO integration\n",
        "USE_GSPO = False  # Set to True to enable GSPO optimization\n",
        "if USE_GSPO:\n",
        "    from datasets import load_dataset\n",
        "    # Example: load agentic examples from a JSONL file (or Parquet if available)\n",
        "    gspo_dataset = load_dataset(\"json\", data_files=\"strawberry-phi.jsonl\", split=\"train\")\n",
        "    # Alternatively, use Parquet:\n",
        "    # gspo_dataset = load_dataset(\"parquet\", data_files=\"strawberry-phi.parquet\", split=\"train\")\n",
        "    print(f\"GSPO dataset loaded with {len(gspo_dataset)} examples\")\n",
        "\n",
        "    # (Optional) Run your GSPO optimization routine here to refine prompt structure\n",
        "    # For demonstration, we simply print a message\n",
        "    print(\"GSPO optimization enabled: refining prompt structure...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsTG_01-02m6"
      },
      "source": [
        "## 6. Fine-Tune the Model with LoRA (with Optional Evaluation Framework)\n",
        "\n",
        "We fine-tune the model using Hugging Face's `TrainingArguments` and the TRL library's `SFTTrainer`. This section includes an evaluation framework that optionally runs a validation step if a validation dataset is available.\n",
        "\n",
        "Key training settings:\n",
        "- **Batch size**: 2 (with gradient accumulation steps to simulate an effective batch size of 8).\n",
        "- **Mixed precision**: Enabled (FP16 or BF16, based on hardware support).\n",
        "- **Training steps**: 50 (for demonstration; use more steps in practice).\n",
        "- **Logging**: Every 5 steps.\n",
        "\n",
        "For a complete evaluation, you can extend this section to include validation metrics, loss curves, and custom evaluation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hWzLyFb02m8"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"outputs\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=50,\n",
        "    warmup_steps=5,\n",
        "    logging_steps=5,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    optim=\"adamw_bnb_fp16\",  # optimizer from bitsandbytes\n",
        "    seed=42,\n",
        "    report_to=[]  # disable external logging\n",
        ")\n",
        "\n",
        "# Initialize the SFT trainer for supervised fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Start training (this may take time depending on GPU and dataset size)\n",
        "trainer.train()\n",
        "\n",
        "# Optional: Evaluation framework\n",
        "def evaluate_model(model, tokenizer, prompts):\n",
        "    model.eval()\n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "        generated_tokens = outputs[0][prompt_len:]\n",
        "        completion = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        print(f\"Prompt: {prompt}\\nCompletion: {completion}\\n{'-'*80}\")\n",
        "\n",
        "# Run evaluation on example prompts\n",
        "evaluation_prompts = [\n",
        "    \"Question: If Mary has 10 candies and gives 4 to John, how many candies remain? Explain your reasoning.\",\n",
        "    \"Question: If A is larger than B and B is larger than C, who is the largest? Reason it out step-by-step.\"\n",
        "]\n",
        "evaluate_model(model, tokenizer, evaluation_prompts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jzdkp702nE"
      },
      "source": [
        "## 7. Save Fine-Tuned Model\n",
        "\n",
        "We can save the fine-tuned model in two ways:\n",
        "\n",
        "- **LoRA Adapter Weights:** Save only the LoRA adapter (the small fine-tuned weights). This is storage-efficient but requires the base model to be loaded later.\n",
        "- **Merged Model:** Merge the LoRA weights into the base model, producing a standalone model (requires more memory during merge).\n",
        "\n",
        "Below, we demonstrate saving the LoRA adapter and (optionally) merging the model for export."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dul59aM402nH"
      },
      "source": [
        "# Save LoRA adapter weights\n",
        "adapter_dir = \"codestral_lora_adapter\"\n",
        "model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "print(f\"LoRA adapter saved to {adapter_dir}/ (contains adapter model weights).\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig0pik_J02nM"
      },
      "source": [
        "# Merge LoRA into base model and save (optional)\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    if isinstance(model, PeftModel):\n",
        "        base_model = model.merge_and_unload()  # merge LoRA weights into the base model\n",
        "    else:\n",
        "        base_model = model\n",
        "    base_model.save_pretrained(\"codestral_finetuned_full\")\n",
        "    tokenizer.save_pretrained(\"codestral_finetuned_full\")\n",
        "    print(\"Merged full model saved to 'codestral_finetuned_full/'\")\n",
        "except Exception as e:\n",
        "    print(f\"Merge failed or not enough memory: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCKayy0U02nO"
      },
      "source": [
        "## 8. Export the Notebook\n",
        "\n",
        "Finally, you can download this notebook as a `.ipynb` file. The code below uses Colab's utility to download the notebook file. If you encounter issues, please save the notebook manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2VXaCpS02nP"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"Deep_Codestral_FineTuning.ipynb\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
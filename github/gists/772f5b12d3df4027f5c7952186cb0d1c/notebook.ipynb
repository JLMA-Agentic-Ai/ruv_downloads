{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "MLflow_H2O_AutoML_DSPy_Pipeline.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/ruvnet/772f5b12d3df4027f5c7952186cb0d1c/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plUbw7rxic2g"
      },
      "source": [
        "# MLflow Pipeline with H2O AutoML and DSPy\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This tutorial demonstrates an end-to-end machine learning pipeline using **H2O's AutoML** for automated model training, **MLflow** for experiment tracking and model deployment, and **DSPy** to illustrate how large language model components can be integrated. We will walk through the steps of setting up the environment, loading data, training multiple models automatically, tracking results with MLflow, and finally deploying the best model. The pipeline is organized in a modular fashion, making it easy to reuse or extend for different datasets or tasks."
      ],
      "id": "plUbw7rxic2g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xltl7OsQic2q"
      },
      "source": [
        "## Installation and Setup\n",
        "\n",
        "To get started, install the required libraries in your Google Colab environment:\n",
        "- **H2O AutoML** – Open-source machine learning library for automatic model selection, training, and evaluation.\n",
        "- **MLflow** – A framework for managing the ML lifecycle, including logging, model versioning, and deployment.\n",
        "- **DSPy** – A declarative framework for self-improving machine learning tasks, replacing PyTorch for our LLM-based component."
      ],
      "id": "Xltl7OsQic2q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvJxtwzCic2s"
      },
      "source": [
        "!pip install h2o mlflow dspy"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "FvJxtwzCic2s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oIuXwDMic2v"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.dspy\n",
        "import h2o\n",
        "import dspy\n",
        "\n",
        "# Initialize H2O\n",
        "h2o.init()\n",
        "\n",
        "# Enable MLflow autologging for DSPy\n",
        "mlflow.dspy.autolog()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4oIuXwDMic2v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUY4qHJjic2x"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "We use the Iris dataset as an example and split it into training and test sets."
      ],
      "id": "gUY4qHJjic2x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_B8nCfeic2x"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name='species')\n",
        "df = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Split into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['species'])\n",
        "\n",
        "# Convert pandas DataFrames to H2O Frames\n",
        "train_hf = h2o.H2OFrame(train_df)\n",
        "test_hf = h2o.H2OFrame(test_df)\n",
        "train_hf['species'] = train_hf['species'].asfactor()\n",
        "test_hf['species'] = test_hf['species'].asfactor()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "t_B8nCfeic2x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY8_DODAic2y"
      },
      "source": [
        "## Model Training with H2O AutoML\n",
        "\n",
        "We train multiple models using H2O's AutoML and track the best-performing model in MLflow."
      ],
      "id": "TY8_DODAic2y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syz4qaujic2z"
      },
      "source": [
        "from h2o.automl import H2OAutoML\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "aml = H2OAutoML(max_models=10, seed=1)\n",
        "with mlflow.start_run(run_name=\"H2O_AutoML_Iris\") as run:\n",
        "    aml.train(x=list(X.columns), y='species', training_frame=train_hf)\n",
        "    best_model = aml.leader\n",
        "    perf = best_model.model_performance(test_hf)\n",
        "    y_true = test_df['species']\n",
        "    y_pred = best_model.predict(test_hf).as_data_frame()['predict']\n",
        "    test_accuracy = accuracy_score(y_true, y_pred)\n",
        "    mlflow.log_param('max_models', 10)\n",
        "    mlflow.log_metric('test_accuracy', test_accuracy)\n",
        "    mlflow.h2o.log_model(best_model, artifact_path='model')\n",
        "run_id = run.info.run_id"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "syz4qaujic2z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9AtfeMiic21"
      },
      "source": [
        "## Model Deployment\n",
        "\n",
        "We now load the best model from MLflow and use it for predictions."
      ],
      "id": "i9AtfeMiic21"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjH9374Pic21"
      },
      "source": [
        "# Load the best model from MLflow\n",
        "loaded_model = mlflow.h2o.load_model(f\"runs:/{run_id}/model\")\n",
        "\n",
        "# Use the model to predict on test data\n",
        "sample = h2o.H2OFrame(test_df.head())\n",
        "predictions = loaded_model.predict(sample)\n",
        "predictions.as_data_frame()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "QjH9374Pic21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMePZUGQic22"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This pipeline demonstrated how to integrate H2O AutoML, MLflow, and DSPy into a self-optimizing machine learning workflow. It covered:\n",
        "- **Data Loading**: Loading and preprocessing structured data.\n",
        "- **AutoML Training**: Automating model selection and training using H2O AutoML.\n",
        "- **Model Tracking**: Using MLflow to log models and track metrics.\n",
        "- **Deployment**: Loading a saved model and running predictions.\n",
        "\n",
        "This modular structure makes it easy to extend the pipeline with new datasets or additional ML models. Future improvements could include hyperparameter tuning, explainability analysis, and deployment through cloud services like AWS or GCP."
      ],
      "id": "eMePZUGQic22"
    }
  ]
}
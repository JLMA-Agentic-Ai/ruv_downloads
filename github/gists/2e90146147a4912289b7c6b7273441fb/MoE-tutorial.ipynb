{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Mixture of Experts (MoE) Model with MergeKit\n",
    "\n",
    "This tutorial walks through the process of creating a Mixture of Experts (MoE) model by ensembling pre-trained expert models using the MergeKit library. The key steps are:\n",
    "\n",
    "1. Introduction to the MoE architecture \n",
    "2. Installing MergeKit\n",
    "3. Selecting pre-trained expert models\n",
    "4. Configuring the MoE model\n",
    "5. Training the MoE model\n",
    "6. Evaluating performance\n",
    "7. Customizing and optimizing the MoE model\n",
    "8. Deploying the trained MoE model\n",
    "\n",
    "## 1. Introduction to the MoE architecture\n",
    "\n",
    "A Mixture of Experts (MoE) model consists of:\n",
    "- Multiple expert models, each specializing in a subset of the data\n",
    "- A gating network that learns which expert to use for each input\n",
    "\n",
    "MoE models can improve performance and efficiency compared to a single large model. MergeKit enables creating MoEs by ensembling pre-trained models (known as frankenMoEs) rather than training from scratch.[1]\n",
    "\n",
    "## 2. Installing MergeKit\n",
    "\n",
    "MergeKit can be installed via pip:\n",
    "```\n",
    "!pip install mergekit\n",
    "```\n",
    "\n",
    "## 3. Selecting pre-trained expert models\n",
    "\n",
    "Choose diverse, high-quality pre-trained models to use as experts in the MoE. For example:\n",
    "- Model 1 (e.g. fine-tuned for chat)\n",
    "- Model 2 (e.g. fine-tuned for code)\n",
    "- Model 3 (e.g. fine-tuned for summarization)\n",
    "\n",
    "## 4. Configuring the MoE model\n",
    "\n",
    "Create a YAML configuration file specifying the base model, experts, and positive prompts for each expert:\n",
    "\n",
    "```yaml\n",
    "base_model: base-model-name\n",
    "experts:\n",
    "  - source_model: model-1-name\n",
    "    positive_prompts: \n",
    "      - \"chat\"\n",
    "      - \"conversation\"\n",
    "  - source_model: model-2-name\n",
    "    positive_prompts:\n",
    "      - \"code\"\n",
    "      - \"program\"\n",
    "  - source_model: model-3-name\n",
    "    positive_prompts:\n",
    "      - \"summarize\"\n",
    "      - \"tldr\"\n",
    "```\n",
    "\n",
    "## 5. Training the MoE model\n",
    "\n",
    "Use the MergeKit `mixtral` branch to create the MoE from the YAML config:\n",
    "\n",
    "```python\n",
    "from mergekit import MergeKit\n",
    "\n",
    "mk = MergeKit(config=\"moe_config.yaml\")\n",
    "moe_model = mk.create_model()\n",
    "```\n",
    "\n",
    "## 6. Evaluating performance \n",
    "\n",
    "Evaluate the MoE model on a held-out test set and compare to individual expert performance.\n",
    "\n",
    "```python\n",
    "moe_score = evaluate(moe_model, test_data) \n",
    "expert1_score = evaluate(expert1_model, test_data)\n",
    "expert2_score = evaluate(expert2_model, test_data)\n",
    "```\n",
    "\n",
    "## 7. Customizing and optimizing\n",
    "\n",
    "Experiment with different expert models, positive prompts, and MergeKit settings to optimize MoE performance for your use case. \n",
    "\n",
    "## 8. Deploying the trained model\n",
    "\n",
    "Export the trained MoE model for deployment and inference:\n",
    "\n",
    "```python\n",
    "moe_model.save(\"trained_moe.pkl\")\n",
    "```\n",
    "\n",
    "The trained MoE model can now be deployed and used for inference, utilizing the strengths of the individual expert models as determined by the gating network.[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
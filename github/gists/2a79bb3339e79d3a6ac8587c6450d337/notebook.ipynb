{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "DSPy_Advanced_NeuroSymbolic_Agentic_Demo.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/ruvnet/2a79bb3339e79d3a6ac8587c6450d337/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-0h_4fi-z23"
      },
      "source": [
        "# Advanced Neuro-Symbolic AI with DSPy: Reasoning, Agents, Graphs, and Explainability\n",
        "\n",
        "This Colab notebook demonstrates advanced AI concepts using the **DSPy** framework, including:\n",
        "- **Neuro-Symbolic Reasoning:** Combining neural methods with symbolic logic (theorem proving, probabilistic logic programming, hybrid models).\n",
        "- **Agentic Planning:** Multi-step decision making with reinforcement learning (RL) and hierarchical planning.\n",
        "- **Graph-Based Learning:** Using Graph Neural Networks (GNNs) and message passing for structured reasoning.\n",
        "- **Explainability & Robustness:** Interpreting model predictions with SHAP and LIME, and testing model robustness.\n",
        "\n",
        "Each section includes code examples and explanations. The notebook is modular, with separate code cells for configuration, model definition, reasoning examples, and evaluation. This ensures smooth execution in Google Colab and easy adaptation to your projects."
      ],
      "id": "D-0h_4fi-z23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9c781iJ-z2_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"Installing required packages...\"\n",
        "pip install -q dspy shap lime gradio networkx sympy transformers\n"
      ],
      "id": "E9c781iJ-z2_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKf3rd9f-z3E"
      },
      "source": [
        "## Configuring DSPy and Language Model\n",
        "\n",
        "First, we install and import DSPy and configure it to use a language model. In this example, we'll integrate DSPy with an OpenRouter API endpoint for model access. We'll set the API key and endpoint, then initialize the DSPy language model client.\n",
        "\n",
        "> **Note:** You need an OpenRouter API key (or an OpenAI API key) to actually run the model calls. Replace the placeholder with your key."
      ],
      "id": "hKf3rd9f-z3E"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import dspy\n",
        "\n",
        "print(\"✅ DSPy Imported\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7mHo8IaAyfZ",
        "outputId": "65ebee09-8309-468d-d4e6-0ef4878242bd"
      },
      "id": "A7mHo8IaAyfZ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DSPy Imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5KujVMj-z3G",
        "outputId": "8103a451-666c-4c64-bbb9-1de2fc819774"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy configured with model: anthropic/claude-3.5-sonnet:beta\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata  # For Colab secret management\n",
        "import dspy\n",
        "\n",
        "# Retrieve the OpenRouter API key from Colab Secrets\n",
        "api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"OPENROUTER_API_KEY not found in Colab Secrets!\")\n",
        "os.environ['OPENROUTER_API_KEY'] = api_key\n",
        "\n",
        "# Clear any Anthropic API key that might cause a conflict\n",
        "if 'ANTHROPIC_API_KEY' in os.environ:\n",
        "    del os.environ['ANTHROPIC_API_KEY']\n",
        "\n",
        "# Force reconfigure DSPy with the OpenRouter model identifier.\n",
        "# Note the model string uses the 'openrouter/' prefix.\n",
        "lm = dspy.LM(\n",
        "    model='anthropic/claude-3.5-sonnet:beta',\n",
        "    api_key=api_key,\n",
        "    api_base='https://openrouter.ai/api/v1',\n",
        "    model_type='chat'\n",
        ")\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "print(\"DSPy configured with model:\", lm.model)\n"
      ],
      "id": "D5KujVMj-z3G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOl5_gke-z3I"
      },
      "source": [
        "## DSPy Text Classification Model\n",
        "\n",
        "We'll define a simple text classification task in DSPy. DSPy uses Signatures to declare input and output fields for a task, and modules like `dsp.Predict` to perform the prediction. Here, we create a signature with an input text and an output sentiment label, then instantiate a predictor module."
      ],
      "id": "tOl5_gke-z3I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wni4x8f-z3J",
        "outputId": "114d6a43-4d13-4bcd-9214-a7d2d44b62e3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy reconfigured with OpenRouter model: openrouter/anthropic/claude-3.5-sonnet:beta\n",
            "Example 1 prediction: Prediction(\n",
            "    sentiment='positive'\n",
            ")\n",
            "Example 2 prediction: Prediction(\n",
            "    sentiment='negative'\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata  # Colab's secret management module\n",
        "import dspy\n",
        "from typing import Literal\n",
        "\n",
        "# Ensure DSPy is configured with the OpenRouter LM.\n",
        "if not hasattr(dspy, 'lm') or dspy.lm is None:\n",
        "    api_key = os.getenv('OPENROUTER_API_KEY')\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENROUTER_API_KEY not found in the environment.\")\n",
        "    # Use the OpenRouter provider by prepending \"openrouter/\" to the model string.\n",
        "    lm = dspy.LM(\n",
        "        model='openrouter/anthropic/claude-3.5-sonnet:beta',  # note the \"openrouter/\" prefix\n",
        "        api_key=api_key,\n",
        "        api_base='https://openrouter.ai/api/v1',\n",
        "        model_type='chat'\n",
        "    )\n",
        "    dspy.configure(lm=lm)\n",
        "    print(\"DSPy reconfigured with OpenRouter model:\", lm.model)\n",
        "else:\n",
        "    print(\"DSPy already configured with model:\", dspy.lm.model)\n",
        "\n",
        "# Define a DSPy Signature for the classification task\n",
        "class SentimentAnalysis(dspy.Signature):\n",
        "    \"\"\"Sentiment classification signature.\"\"\"\n",
        "    text: str = dspy.InputField(description=\"Input text to classify\")\n",
        "    sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField(description=\"Predicted sentiment\")\n",
        "\n",
        "# Create a DSPy predictor for sentiment analysis\n",
        "sentiment_classifier = dspy.Predict(SentimentAnalysis)\n",
        "\n",
        "# Test the classifier with a couple of examples\n",
        "try:\n",
        "    result1 = sentiment_classifier(text=\"I love this new phone, it's fantastic!\")\n",
        "    result2 = sentiment_classifier(text=\"The movie was a bit dull and boring.\")\n",
        "    print(\"Example 1 prediction:\", result1)\n",
        "    print(\"Example 2 prediction:\", result2)\n",
        "except Exception as e:\n",
        "    print(\"Model call failed:\", e)\n"
      ],
      "id": "7Wni4x8f-z3J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pct2Zsu-z3K"
      },
      "source": [
        "## Advanced Neuro-Symbolic Reasoning\n",
        "\n",
        "Neuro-symbolic AI combines neural networks with symbolic reasoning to leverage the strengths of both. In this section, we demonstrate:\n",
        "- **Theorem Proving:** Using a symbolic logic engine to prove logical statements.\n",
        "- **Probabilistic Logic Programming:** Reasoning under uncertainty with probabilities.\n",
        "- **Hybrid Models:** Integrating neural outputs with symbolic rules for improved consistency.\n"
      ],
      "id": "4Pct2Zsu-z3K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo0f5uht-z3M",
        "outputId": "fd242364-b2b6-4751-bae1-c251f1f5454f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premises: A is true and A -> B is true\n",
            "Conclusion B is implied: True\n"
          ]
        }
      ],
      "source": [
        "# Theorem Proving Example\n",
        "from sympy import symbols, Implies, And, Not, satisfiable\n",
        "\n",
        "# Define propositional symbols\n",
        "A, B = symbols('A B')\n",
        "# Define premises: A is true and A implies B\n",
        "premises = And(A, Implies(A, B))\n",
        "# Check if premises imply B\n",
        "implication_holds = not satisfiable(And(premises, Not(B)))\n",
        "\n",
        "print(\"Premises: A is true and A -> B is true\")\n",
        "print(\"Conclusion B is implied:\", implication_holds)"
      ],
      "id": "Vo0f5uht-z3M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2QcSVRT-z3O"
      },
      "source": [
        "### Probabilistic Logic Programming\n",
        "\n",
        "We can extend symbolic reasoning with probabilities to handle uncertainty. In this example, we compute the probability of rain given that the ground is wet using Bayes' theorem."
      ],
      "id": "g2QcSVRT-z3O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9uqftkp-z3P",
        "outputId": "83759033-717c-45d0-d8e8-caa7e8b92ad4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(A) = 0.5, P(B|A) = 0.8, P(B|¬A) = 0.1\n",
            "P(A|B) = 0.89\n"
          ]
        }
      ],
      "source": [
        "P_A = 0.5\n",
        "P_B_given_A = 0.8\n",
        "P_B_given_notA = 0.1\n",
        "\n",
        "P_notA = 1 - P_A\n",
        "P_B = P_B_given_A * P_A + P_B_given_notA * P_notA\n",
        "P_A_given_B = (P_B_given_A * P_A) / P_B\n",
        "\n",
        "print(f\"P(A) = {P_A}, P(B|A) = {P_B_given_A}, P(B|¬A) = {P_B_given_notA}\")\n",
        "print(f\"P(A|B) = {P_A_given_B:.2f}\")"
      ],
      "id": "s9uqftkp-z3P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUkSXcwS-z3R"
      },
      "source": [
        "### Hybrid Neuro-Symbolic Reasoning\n",
        "\n",
        "In a hybrid approach, the system uses neural networks for general understanding and symbolic logic for precise inference. Below, we take the sentiment classifier's prediction and adjust it with a simple rule."
      ],
      "id": "fUkSXcwS-z3R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCMRFLJA-z3S",
        "outputId": "2f5d8749-403d-4ae1-e1b5-542f34468125"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model predicted sentiment: negative\n",
            "No rule applied.\n",
            "Final sentiment: negative\n"
          ]
        }
      ],
      "source": [
        "# Hybrid reasoning: Use the neural model output and apply a symbolic rule\n",
        "text = \"The movie was not good.\"\n",
        "try:\n",
        "    model_pred = sentiment_classifier(text=text)\n",
        "    predicted_label = model_pred.sentiment if hasattr(model_pred, 'sentiment') else str(model_pred)\n",
        "except Exception as e:\n",
        "    predicted_label = \"neutral\"\n",
        "\n",
        "print(\"Model predicted sentiment:\", predicted_label)\n",
        "\n",
        "adjusted_label = predicted_label\n",
        "if \"not good\" in text.lower() and adjusted_label != \"negative\":\n",
        "    adjusted_label = \"negative\"\n",
        "    print(\"Rule applied: 'not good' -> sentiment set to negative\")\n",
        "else:\n",
        "    print(\"No rule applied.\")\n",
        "\n",
        "print(\"Final sentiment:\", adjusted_label)"
      ],
      "id": "uCMRFLJA-z3S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njmMydGQ-z3T"
      },
      "source": [
        "## Agentic Extensions: Multi-step Planning and Hierarchical Decision-Making\n",
        "\n",
        "Next, we explore agentic capabilities where an AI agent makes decisions over multiple steps:\n",
        "- **Reinforcement Learning (RL):** The agent learns via trial and error, receiving rewards for good actions.\n",
        "- **Hierarchical Planning:** Decisions are made at different abstraction levels, enabling the agent to decompose complex tasks into sub-tasks.\n"
      ],
      "id": "njmMydGQ-z3T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9bzdbF0-z3T",
        "outputId": "31df04b3-a805-4290-c6ef-cdfa9f947e96"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned plan from state 0 to goal: [2, 2, 2, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Simple Q-learning example: environment with state 0 to goal 10\n",
        "goal = 10\n",
        "actions = [1, 2]\n",
        "Q = {}\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "for episode in range(100):\n",
        "    state = 0\n",
        "    while state < goal:\n",
        "        action = random.choice(actions)\n",
        "        next_state = state + action\n",
        "        if next_state > goal:\n",
        "            next_state = state\n",
        "            reward = -1.0\n",
        "        elif next_state == goal:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = -0.01\n",
        "        old_Q = Q.get((state, action), 0.0)\n",
        "        future_vals = [Q.get((next_state, a), 0.0) for a in actions]\n",
        "        max_future = max(future_vals) if future_vals else 0.0\n",
        "        new_Q = old_Q + alpha * (reward + gamma * max_future - old_Q)\n",
        "        Q[(state, action)] = new_Q\n",
        "        state = next_state\n",
        "\n",
        "state = 0\n",
        "plan = []\n",
        "while state < goal and len(plan) < 20:\n",
        "    best_action = max(actions, key=lambda a: Q.get((state, a), 0.0))\n",
        "    plan.append(best_action)\n",
        "    state += best_action\n",
        "\n",
        "print(\"Learned plan from state 0 to goal:\", plan)"
      ],
      "id": "b9bzdbF0-z3T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vln2yF1o-z3U"
      },
      "source": [
        "## Graph-Based Reasoning with GNNs and Message Passing\n",
        "\n",
        "We simulate graph-based reasoning using a simple message passing example with a graph. In a real-world scenario, a Graph Neural Network (GNN) would update node representations by exchanging messages with neighbors.\n"
      ],
      "id": "Vln2yF1o-z3U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzPc6sOd-z3U",
        "outputId": "2172eb90-9353-4ca3-fa1a-6a15e14f0eb9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial features: {0: 10.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
            "Features after one message-passing step: {0: 0.0, 1: 5.0, 2: 0.0, 3: 0.0}\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.path_graph(4)\n",
        "features = {0: 10.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
        "print(\"Initial features:\", features)\n",
        "\n",
        "new_features = {}\n",
        "for node in G.nodes:\n",
        "    neighbor_vals = [features[n] for n in G.neighbors(node)]\n",
        "    new_features[node] = sum(neighbor_vals) / len(neighbor_vals) if neighbor_vals else features[node]\n",
        "\n",
        "print(\"Features after one message-passing step:\", new_features)"
      ],
      "id": "fzPc6sOd-z3U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw0vRYfu-z3V"
      },
      "source": [
        "## Evaluation and Explainability\n",
        "\n",
        "To better understand model decisions, we use explainability tools such as SHAP and LIME. In addition, we perform robustness testing by perturbing inputs and observing changes in model predictions."
      ],
      "id": "Yw0vRYfu-z3V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKugeOFt-z3V",
        "outputId": "c1310322-b2a0-4fd1-bb9a-5cadd6f77f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from google.colab import userdata  # Colab's secret management module\n",
        "\n",
        "# Detect device: use GPU (device=0) if available, otherwise CPU (device=-1)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Device set to use {'GPU' if device != -1 else 'CPU'}\")\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab Secrets using key \"HUGGINGFACE_API_KEY\"\n",
        "try:\n",
        "    hf_token = userdata.get(\"HUGGINGFACE_API_KEY\")\n",
        "except Exception as e:\n",
        "    print(\"HUGGINGFACE_API_KEY not found in Colab Secrets:\", e)\n",
        "    hf_token = None\n",
        "\n",
        "# If a token is found, set it as an environment variable for the Hugging Face Hub.\n",
        "if hf_token:\n",
        "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
        "else:\n",
        "    print(\"Warning: HUGGINGFACE_API_KEY not set. Using public model access.\")\n",
        "\n",
        "# Instantiate the sentiment-analysis pipeline with explicit model, revision, and device.\n",
        "hf_classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    revision=\"714eb0f\",\n",
        "    return_all_scores=True,\n",
        "    device=device  # Use GPU if available; otherwise, CPU\n",
        ")\n",
        "\n",
        "class_names = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "\n",
        "def predict_proba(texts):\n",
        "    results = hf_classifier(texts)\n",
        "    probas = []\n",
        "    for res in results:\n",
        "        # Create a dictionary mapping labels to scores\n",
        "        score_dict = {d['label'].upper(): d['score'] for d in res}\n",
        "        # Order probabilities according to class_names order\n",
        "        probas.append([score_dict.get(\"NEGATIVE\", 0), score_dict.get(\"POSITIVE\", 0)])\n",
        "    return np.array(probas)\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "text_instance = \"I am not sure if I really like this product.\"\n",
        "exp = explainer.explain_instance(text_instance, predict_proba, num_features=6)\n",
        "\n",
        "print(f\"Input: {text_instance}\")\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    explanation = exp.as_list(label=class_idx)\n",
        "    print(f\"Top features for {class_name}: {explanation}\")\n"
      ],
      "id": "aKugeOFt-z3V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the SHAP library to explain which words in a given text instance have the greatest impact on the sentiment classifier’s output. Here’s a breakdown of what the code does:\n",
        "\n",
        "1. **Initialize the Explainer:**  \n",
        "   An explainer is created using `shap.Explainer(hf_classifier)`, where `hf_classifier` is your pre-trained Hugging Face sentiment analysis pipeline. This explainer is set up to compute SHAP values for the model’s predictions.\n",
        "\n",
        "2. **Compute SHAP Values:**  \n",
        "   The code calls `explainer([text_instance])` to compute SHAP values for the provided text (stored in `text_instance`). The result, `shap_values`, contains both the tokenized data (i.e. the words from the input) and the SHAP values, which quantify the contribution of each word to the prediction.\n",
        "\n",
        "3. **Extract Data and Values:**  \n",
        "   From the first (and only) explanation in `shap_values`, `data` holds the list of words (or tokens) and `values` holds a list of corresponding SHAP values for each sentiment class (here, “NEGATIVE” and “POSITIVE”).\n",
        "\n",
        "4. **Identify Top Influential Words:**  \n",
        "   For each sentiment class, the code pairs each word with its corresponding SHAP value using `zip(data, values[i])`. It then sorts these pairs by the absolute value of the SHAP contribution (i.e., words that have the largest impact on the model’s output are prioritized) and selects the top five words.\n",
        "\n",
        "5. **Display the Results:**  \n",
        "   Finally, the top five word-impact pairs for each sentiment class are printed. Each pair shows the word and its SHAP value (formatted to three decimal places), which indicates whether the word pushes the prediction towards a particular sentiment (with a positive or negative sign).\n",
        "\n",
        "This explanation helps you understand which features (words) are most influential in the model's prediction, thereby making the sentiment analysis process more interpretable."
      ],
      "metadata": {
        "id": "ew05pc0UDusL"
      },
      "id": "ew05pc0UDusL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "kZlou6YC-z3W",
        "outputId": "fc44d7b9-e99a-4ab4-e7f6-00b5268ae2f1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'use_auth_token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f6ca4cbe7e8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_instance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    126\u001b[0m                  outputs=None, silent=False):\n\u001b[1;32m    127\u001b[0m         \u001b[0;34m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/explainers/_partition.py\u001b[0m in \u001b[0;36mexplain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# if not fixed background or no base value assigned then compute base value for a row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_base_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fixed_background\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_base_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the zero index param tells the masked model what the baseline is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mf11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mm00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m_full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mjoined_masked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_masked_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjoined_masked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0m_assert_output_input_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_masked_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/models/_transformers_pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, strings)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shap.models.TransformersPipeline expects a list of strings not a single string!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpipeline_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[1;32m    158\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 )\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;34m' dictionary `{\"text\": \"My text\", \"text_pair\": \"My pair\"}` in order to send a text pair.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             )\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2976\u001b[0m             )\n\u001b[1;32m   2977\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2978\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2979\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2980\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3052\u001b[0m         )\n\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3054\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3055\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3056\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    612\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'use_auth_token'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from google.colab import userdata  # Colab secret management module\n",
        "import shap\n",
        "\n",
        "# Detect device: use GPU if available, otherwise CPU.\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Device set to use {'GPU' if device != -1 else 'CPU'}\")\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab Secrets using the key \"HUGGINGFACE_API_KEY\"\n",
        "try:\n",
        "    hf_token = userdata.get(\"HUGGINGFACE_API_KEY\")\n",
        "except Exception as e:\n",
        "    print(\"HUGGINGFACE_API_KEY not found in Colab Secrets:\", e)\n",
        "    hf_token = None\n",
        "\n",
        "if hf_token:\n",
        "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
        "else:\n",
        "    print(\"Warning: HUGGINGFACE_API_KEY not set. Using public model access.\")\n",
        "\n",
        "# Instantiate the sentiment-analysis pipeline with explicit model, revision, and device.\n",
        "hf_classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    revision=\"714eb0f\",\n",
        "    return_all_scores=True,\n",
        "    device=device  # runs on GPU if available, otherwise on CPU\n",
        ")\n",
        "\n",
        "# Patch the classifier's tokenizer to remove the use_auth_token attribute (if it exists)\n",
        "if hasattr(hf_classifier.tokenizer, \"use_auth_token\"):\n",
        "    hf_classifier.tokenizer.use_auth_token = None\n",
        "\n",
        "class_names = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "\n",
        "def predict_proba(texts):\n",
        "    results = hf_classifier(texts)\n",
        "    probas = []\n",
        "    for res in results:\n",
        "        # Create a dictionary mapping labels to scores\n",
        "        score_dict = {d['label'].upper(): d['score'] for d in res}\n",
        "        # Order probabilities according to class_names\n",
        "        probas.append([score_dict.get(\"NEGATIVE\", 0), score_dict.get(\"POSITIVE\", 0)])\n",
        "    return np.array(probas)\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "text_instance = \"I am not sure if I really like this product.\"\n",
        "\n",
        "# Compute SHAP values for the given text instance.\n",
        "try:\n",
        "    shap_values = explainer.explain_instance(text_instance, predict_proba, num_features=6)\n",
        "    data = shap_values[0].data\n",
        "    values = shap_values[0].values\n",
        "\n",
        "    print(f\"Input: {text_instance}\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        word_shap_pairs = list(zip(data, values[i]))\n",
        "        top_pairs = sorted(word_shap_pairs, key=lambda x: abs(x[1]), reverse=True)[:5]\n",
        "        contrib = \", \".join([f\"'{w}': {val:+.3f}\" for w, val in top_pairs])\n",
        "        print(f\"{class_name} class top impacts: {contrib}\")\n",
        "except Exception as e:\n",
        "    print(\"SHAP explanation failed:\", e)\n"
      ],
      "id": "kZlou6YC-z3W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54oGVzk_-z3X"
      },
      "source": [
        "### Robustness Testing\n",
        "\n",
        "We test robustness by perturbing sample texts. Perturbations include introducing typos and removing vowels. We then observe how the model's prediction changes."
      ],
      "id": "54oGVzk_-z3X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV1fCse_-z3X"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def add_typos(text, n_typos=2):\n",
        "    text_chars = list(text)\n",
        "    for _ in range(n_typos):\n",
        "        idx = random.randrange(len(text_chars))\n",
        "        text_chars[idx] = chr((ord(text_chars[idx]) + 1) % 26 + 97) if text_chars[idx].isalpha() else text_chars[idx]\n",
        "    return \"\".join(text_chars)\n",
        "\n",
        "orig_text_pos = \"This movie is absolutely wonderful, I loved every moment of it.\"\n",
        "orig_text_neg = \"I do not like this product at all, it's terrible.\"\n",
        "\n",
        "pert_text_pos_typo = add_typos(orig_text_pos, n_typos=3)\n",
        "pert_text_neg_typo = add_typos(orig_text_neg, n_typos=3)\n",
        "pert_text_pos_novowel = re.sub(r'[AEIOUaeiou]', '', orig_text_pos)\n",
        "pert_text_neg_novowel = re.sub(r'[AEIOUaeiou]', '', orig_text_neg)\n",
        "\n",
        "texts = [orig_text_pos, pert_text_pos_typo, pert_text_pos_novowel,\n",
        "         orig_text_neg, pert_text_neg_typo, pert_text_neg_novowel]\n",
        "predictions = hf_classifier(texts)\n",
        "\n",
        "labels = [pred[0]['label'] for pred in predictions]\n",
        "\n",
        "print(\"Original positive prediction:\", labels[0])\n",
        "print(\"Positive with typos:\", labels[1])\n",
        "print(\"Positive without vowels:\", labels[2])\n",
        "print(\"Original negative prediction:\", labels[3])\n",
        "print(\"Negative with typos:\", labels[4])\n",
        "print(\"Negative without vowels:\", labels[5])\n"
      ],
      "id": "dV1fCse_-z3X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uiyYWYB-z3Y"
      },
      "source": [
        "## Interactive Demo with Gradio\n",
        "\n",
        "The following Gradio app provides an interactive interface with two tabs:\n",
        "- **Demo Tab:** Enter custom text to get sentiment predictions from the DSPy model and a reference transformer model.\n",
        "- **Evaluation Tab:** Select sample text and a perturbation method to see the impact on model predictions.\n"
      ],
      "id": "6uiyYWYB-z3Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvgjFx4Z-z3Y"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def classify_text(text):\n",
        "    try:\n",
        "        dsp_pred = sentiment_classifier(text=text)\n",
        "        dsp_label = dsp_pred.sentiment if hasattr(dsp_pred, 'sentiment') else str(dsp_pred)\n",
        "    except Exception:\n",
        "        dsp_label = \"N/A (model not available)\"\n",
        "    hf_result = hf_classifier(text)[0]\n",
        "    label = hf_result['label']\n",
        "    score = hf_result['score']\n",
        "    return f\"DSPy Model: {dsp_label}\", f\"Transformer: {label} (conf {score:.2f})\"\n",
        "\n",
        "def eval_perturb(text, perturbation):\n",
        "    if perturbation == \"Add Typos\":\n",
        "        pert_text = add_typos(text, n_typos=3)\n",
        "    elif perturbation == \"Remove Vowels\":\n",
        "        pert_text = re.sub(r'[AEIOUaeiou]', '', text)\n",
        "    else:\n",
        "        pert_text = text\n",
        "    orig = hf_classifier(text)[0]\n",
        "    pert = hf_classifier(pert_text)[0]\n",
        "    return (f\"Original: {orig['label']} (conf {orig['score']:.2f})\",\n",
        "            f\"Perturbed: {pert['label']} (conf {pert['score']:.2f})\",\n",
        "            f\"Modified Text: {pert_text}\")\n",
        "\n",
        "demo_interface = gr.Interface(fn=classify_text,\n",
        "                              inputs=gr.Textbox(label=\"Input Text\"),\n",
        "                              outputs=[gr.Textbox(label=\"DSPy Output\"), gr.Textbox(label=\"Transformer Output\")],\n",
        "                              title=\"Sentiment Classification Demo\",\n",
        "                              description=\"Enter a sentence to classify its sentiment.\")\n",
        "\n",
        "eval_interface = gr.Interface(fn=eval_perturb,\n",
        "                              inputs=[gr.Dropdown([orig_text_pos, orig_text_neg], label=\"Sample Text\"),\n",
        "                                      gr.Dropdown([\"Add Typos\", \"Remove Vowels\"], label=\"Perturbation\")],\n",
        "                              outputs=[gr.Textbox(label=\"Original Prediction\"), gr.Textbox(label=\"Perturbed Prediction\"), gr.Textbox(label=\"Modified Text\")],\n",
        "                              title=\"Robustness Evaluation\",\n",
        "                              description=\"Apply perturbations to a sample text and see the effect on the prediction.\")\n",
        "\n",
        "gradio_app = gr.TabbedInterface([demo_interface, eval_interface], [\"Demo\", \"Evaluation\"])\n",
        "gradio_app.launch()"
      ],
      "id": "FvgjFx4Z-z3Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkeWB6Wy-z3Z"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated an advanced DSPy workflow integrating neuro-symbolic reasoning, multi-step agentic planning with reinforcement learning and hierarchical decision-making, graph-based reasoning with message passing, and explainability via SHAP and LIME. The interactive Gradio UI provides both demo and evaluation capabilities, making the system modular and deployable in Google Colab.\n",
        "\n",
        "Feel free to extend this notebook with your own data, additional modules, and more detailed evaluation metrics.\n",
        "\n",
        "Happy fine tuning and reasoning!"
      ],
      "id": "mkeWB6Wy-z3Z"
    }
  ]
}
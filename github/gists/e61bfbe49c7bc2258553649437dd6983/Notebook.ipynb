{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# MIND: Modular Integration of Neural and Deductive Systems for Neuro-Symbolic AI \n",
    "\n",
    "This notebook demonstrates an advanced neuro-symbolic framework that integrates a neural network with symbolic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "in-depth-introduction",
   "metadata": {},
   "source": [
    "## In-Depth Introduction\n",
    "\n",
    "Neuro-symbolic AI is an emerging paradigm that aims to combine the pattern recognition strengths of deep learning with the explicit, rule-based reasoning capabilities of symbolic systems. The **MIND** (Modular Integration of Neural and Deductive systems) framework exemplifies this integration by allowing a neural model to learn directly from data while also obeying predefined logical constraints.\n",
    "\n",
    "In this notebook, we illustrate the approach using a toy example where the goal is to predict a symmetric friendship relation among entities. The key idea is to enforce the symmetry rule—if entity A is a friend of entity B, then entity B must also be a friend of entity A—by adding a logic loss to the training process. This integration helps ensure that the model's predictions are consistent with domain knowledge and can lead to improved performance and interpretability.\n",
    "\n",
    "### Why Neuro-Symbolic Integration?\n",
    "\n",
    "- **Interpretability:** By embedding symbolic rules into the training process, the model’s decisions become more transparent and easier to explain.\n",
    "- **Robustness:** The addition of logic constraints helps reduce errors that might arise solely from data-driven learning, especially when data is limited or noisy.\n",
    "- **Generalizability:** This approach can be extended to more complex scenarios, enabling the development of AI systems that are both data‑driven and knowledge‑driven.\n",
    "\n",
    "This notebook is designed for researchers, PhD students, and practitioners interested in exploring neuro‑symbolic methods and applying them to real-world AI challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features-usages",
   "metadata": {},
   "source": [
    "## Features and Usages\n",
    "\n",
    "This updated notebook offers a range of features and demonstrates various usages of the MIND framework:\n",
    "\n",
    "1. **Installation and Setup**\n",
    "   - Easily install the required libraries (PyTorch, NumPy, Matplotlib, ipywidgets) to begin your experiments.\n",
    "\n",
    "2. **Interactive Parameters UI**\n",
    "   - Adjust key hyperparameters such as learning rate, lambda for the logic loss, number of epochs, and the probability of friendship using interactive sliders.\n",
    "\n",
    "3. **Step-by-Step Tutorial**\n",
    "   - Follow a detailed tutorial that guides you through dataset generation, model definition, training (both baseline and with logic constraints), and result visualization.\n",
    "\n",
    "4. **Neuro-Symbolic Integration**\n",
    "   - Learn how to combine neural predictions with symbolic logical constraints to enforce domain-specific rules (e.g., the symmetry of friendships).\n",
    "\n",
    "5. **Visualization**\n",
    "   - Visualize training progress, test accuracy, and the number of logical rule violations across epochs.\n",
    "\n",
    "6. **Advanced Options**\n",
    "   - Explore optional advanced configurations such as additional symbolic rules (e.g., transitivity), multi-task learning, or different model architectures.\n",
    "\n",
    "This notebook is intended to be both a practical tutorial and a starting point for further research into neuro‑symbolic methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation cell\n",
    "!pip install torch torchvision ipywidgets matplotlib numpy\n",
    "\n",
    "# For JupyterLab users, you might need to enable the widget extension:\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameters-ui-description",
   "metadata": {},
   "source": [
    "## Parameters UI\n",
    "\n",
    "Use the interactive UI below to adjust key hyperparameters. You can change the **learning rate**, **lambda** for the logic loss, **number of epochs**, and the **friendship probability** for generating the toy dataset. The parameters will automatically update and print the current settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameters-ui",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define interactive widgets for hyperparameters\n",
    "learning_rate_widget = widgets.FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01, description='Learning Rate:')\n",
    "lambda_logic_widget = widgets.FloatSlider(value=1.0, min=0.0, max=10.0, step=0.1, description='Lambda Logic:')\n",
    "epochs_widget = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Epochs:')\n",
    "friend_prob_widget = widgets.FloatSlider(value=0.3, min=0.0, max=1.0, step=0.05, description='Friend Prob:')\n",
    "\n",
    "display(learning_rate_widget, lambda_logic_widget, epochs_widget, friend_prob_widget)\n",
    "\n",
    "# Store parameters in a dictionary for later use\n",
    "params = {\n",
    "    'learning_rate': learning_rate_widget.value,\n",
    "    'lambda_logic': lambda_logic_widget.value,\n",
    "    'epochs': epochs_widget.value,\n",
    "    'friend_prob': friend_prob_widget.value\n",
    "}\n",
    "\n",
    "# Update parameters when sliders change\n",
    "def update_params(change):\n",
    "    params['learning_rate'] = learning_rate_widget.value\n",
    "    params['lambda_logic'] = lambda_logic_widget.value\n",
    "    params['epochs'] = epochs_widget.value\n",
    "    params['friend_prob'] = friend_prob_widget.value\n",
    "    print('Updated parameters:', params)\n",
    "\n",
    "learning_rate_widget.observe(update_params, names='value')\n",
    "lambda_logic_widget.observe(update_params, names='value')\n",
    "epochs_widget.observe(update_params, names='value')\n",
    "friend_prob_widget.observe(update_params, names='value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-overview",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial\n",
    "\n",
    "Follow the cells below to:\n",
    "\n",
    "1. **Generate a Toy Dataset**: Create a set of 10 entities with randomly assigned symmetric friendship relations.\n",
    "2. **Define the Neural Model**: Build a neural network that predicts friendship probability from learned embeddings.\n",
    "3. **Train the Baseline Model**: Train using standard binary cross-entropy loss (data only).\n",
    "4. **Train the MIND Model**: Train with an additional logic loss that enforces the symmetry constraint.\n",
    "5. **Visualize the Results**: Compare test accuracy and logical rule violations for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toy-dataset-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define 10 entities labeled A, B, C, ... J\n",
    "entities = [chr(i) for i in range(65, 75)]\n",
    "num_entities = len(entities)\n",
    "\n",
    "# Use the friend probability from the parameters UI\n",
    "friend_prob = params.get('friend_prob', 0.3)\n",
    "friend_matrix = np.zeros((num_entities, num_entities), dtype=int)\n",
    "for i in range(num_entities):\n",
    "    for j in range(i+1, num_entities):\n",
    "        if np.random.rand() < friend_prob:\n",
    "            friend_matrix[i, j] = 1\n",
    "            friend_matrix[j, i] = 1\n",
    "\n",
    "# No self-friendships\n",
    "np.fill_diagonal(friend_matrix, 0)\n",
    "\n",
    "# Split each unique undirected pair into training and testing (opposite directions)\n",
    "train_pairs = []\n",
    "train_labels = []\n",
    "test_pairs = []\n",
    "test_labels = []\n",
    "for i in range(num_entities):\n",
    "    for j in range(i+1, num_entities):\n",
    "        label = friend_matrix[i, j]\n",
    "        if np.random.rand() < 0.5:\n",
    "            train_pairs.append((i, j)); train_labels.append(label)\n",
    "            test_pairs.append((j, i)); test_labels.append(label)\n",
    "        else:\n",
    "            train_pairs.append((j, i)); train_labels.append(label)\n",
    "            test_pairs.append((i, j)); test_labels.append(label)\n",
    "\n",
    "train_pairs = np.array(train_pairs)\n",
    "train_labels = np.array(train_labels)\n",
    "test_pairs = np.array(test_pairs)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Total entities: {num_entities} -> {entities}\")\n",
    "num_friend_pairs = int(friend_matrix.sum() / 2)\n",
    "print(f\"Generated {num_friend_pairs} friendship pairs (ground truth is symmetric).\")\n",
    "\n",
    "print(\"Example training samples:\")\n",
    "for idx in range(min(5, len(train_pairs))):\n",
    "    i, j = train_pairs[idx]\n",
    "    label = train_labels[idx]\n",
    "    relation = \"Friend\" if label == 1 else \"Not Friend\"\n",
    "    print(f\"  {entities[i]} -> {entities[j]} : {relation}\")\n",
    "\n",
    "print(\"Example testing samples:\")\n",
    "for idx in range(min(5, len(test_pairs))):\n",
    "    i, j = test_pairs[idx]\n",
    "    label = test_labels[idx]\n",
    "    relation = \"Friend\" if label == 1 else \"Not Friend\"\n",
    "    print(f\"  {entities[i]} -> {entities[j]} : {relation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-definition-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class FriendPredictor(nn.Module):\n",
    "    def __init__(self, num_entities, embed_dim=16, hidden_dim=32):\n",
    "        super(FriendPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_entities, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, i_indices, j_indices):\n",
    "        emb_i = self.embedding(i_indices)\n",
    "        emb_j = self.embedding(j_indices)\n",
    "        concat = torch.cat([emb_i, emb_j], dim=-1)\n",
    "        hidden = torch.relu(self.fc1(concat))\n",
    "        logit = self.fc2(hidden).squeeze(-1)\n",
    "        return logit\n",
    "\n",
    "# Instantiate two models: baseline and MIND\n",
    "num_entities = len(entities)\n",
    "model_baseline = FriendPredictor(num_entities)\n",
    "model_mind = FriendPredictor(num_entities)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_baseline = model_baseline.to(device)\n",
    "model_mind = model_mind.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-baseline-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and testing data as torch tensors\n",
    "train_i = torch.tensor(train_pairs[:, 0], dtype=torch.long).to(device)\n",
    "train_j = torch.tensor(train_pairs[:, 1], dtype=torch.long).to(device)\n",
    "train_y = torch.tensor(train_labels, dtype=torch.float).to(device)\n",
    "test_i  = torch.tensor(test_pairs[:, 0], dtype=torch.long).to(device)\n",
    "test_j  = torch.tensor(test_pairs[:, 1], dtype=torch.long).to(device)\n",
    "test_y  = torch.tensor(test_labels, dtype=torch.float).to(device)\n",
    "\n",
    "# Use parameters from the UI\n",
    "epochs = params.get('epochs', 50)\n",
    "learning_rate = params.get('learning_rate', 0.1)\n",
    "\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "baseline_acc_history = []\n",
    "baseline_violations_history = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_baseline.train()\n",
    "    optimizer_baseline.zero_grad()\n",
    "    logits = model_baseline(train_i, train_j)\n",
    "    loss = loss_fn(logits, train_y)\n",
    "    loss.backward()\n",
    "    optimizer_baseline.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model_baseline.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model_baseline(test_i, test_j)\n",
    "        test_probs = torch.sigmoid(test_logits)\n",
    "        preds = (test_probs.cpu().numpy() >= 0.5).astype(int)\n",
    "        labels = test_y.cpu().numpy().astype(int)\n",
    "        test_acc = (preds == labels).mean()\n",
    "        baseline_acc_history.append(test_acc)\n",
    "        \n",
    "        # Count symmetry violations over all unique pairs\n",
    "        inconsistencies = 0\n",
    "        for a in range(num_entities):\n",
    "            for b in range(a + 1, num_entities):\n",
    "                prob_ab = torch.sigmoid(model_baseline(torch.tensor([a]).to(device), torch.tensor([b]).to(device))).item()\n",
    "                prob_ba = torch.sigmoid(model_baseline(torch.tensor([b]).to(device), torch.tensor([a]).to(device))).item()\n",
    "                if (prob_ab > 0.5 and prob_ba < 0.5) or (prob_ab < 0.5 and prob_ba > 0.5):\n",
    "                    inconsistencies += 1\n",
    "        baseline_violations_history.append(inconsistencies)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Baseline] Epoch {epoch}/{epochs} - Loss: {loss.item():.4f}, Test Acc: {test_acc*100:.1f}%, Violations: {inconsistencies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-mind-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MIND model with logic loss enforcing symmetry\n",
    "epochs = params.get('epochs', 50)\n",
    "learning_rate = params.get('learning_rate', 0.1)\n",
    "lambda_logic = params.get('lambda_logic', 1.0)\n",
    "\n",
    "optimizer_mind = torch.optim.Adam(model_mind.parameters(), lr=learning_rate)\n",
    "\n",
    "mind_acc_history = []\n",
    "mind_violations_history = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_mind.train()\n",
    "    optimizer_mind.zero_grad()\n",
    "    logits = model_mind(train_i, train_j)\n",
    "    data_loss = loss_fn(logits, train_y)\n",
    "\n",
    "    # Compute logic loss: enforce symmetry between Friend(X,Y) and Friend(Y,X) for all unique pairs\n",
    "    pairs_i = []\n",
    "    pairs_j = []\n",
    "    for a in range(num_entities):\n",
    "        for b in range(a + 1, num_entities):\n",
    "            pairs_i.extend([a, b])\n",
    "            pairs_j.extend([b, a])\n",
    "    pairs_i = torch.tensor(pairs_i, dtype=torch.long).to(device)\n",
    "    pairs_j = torch.tensor(pairs_j, dtype=torch.long).to(device)\n",
    "    logits_all = model_mind(pairs_i, pairs_j)\n",
    "    probs_all = torch.sigmoid(logits_all)\n",
    "    # Reshape to [N_pairs, 2] so that each unique pair has two predictions\n",
    "    probs_matrix = probs_all.view(-1, 2)\n",
    "    diff = probs_matrix[:, 0] - probs_matrix[:, 1]\n",
    "    logic_loss = torch.mean(diff * diff)\n",
    "\n",
    "    loss = data_loss + lambda_logic * logic_loss\n",
    "    loss.backward()\n",
    "    optimizer_mind.step()\n",
    "\n",
    "    model_mind.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model_mind(test_i, test_j)\n",
    "        test_probs = torch.sigmoid(test_logits)\n",
    "        preds = (test_probs.cpu().numpy() >= 0.5).astype(int)\n",
    "        labels = test_y.cpu().numpy().astype(int)\n",
    "        test_acc = (preds == labels).mean()\n",
    "        mind_acc_history.append(test_acc)\n",
    "\n",
    "        # Count symmetry violations on all unique pairs\n",
    "        inconsistencies = 0\n",
    "        for a in range(num_entities):\n",
    "            for b in range(a + 1, num_entities):\n",
    "                prob_ab = torch.sigmoid(model_mind(torch.tensor([a]).to(device), torch.tensor([b]).to(device))).item()\n",
    "                prob_ba = torch.sigmoid(model_mind(torch.tensor([b]).to(device), torch.tensor([a]).to(device))).item()\n",
    "                if (prob_ab > 0.5 and prob_ba < 0.5) or (prob_ab < 0.5 and prob_ba > 0.5):\n",
    "                    inconsistencies += 1\n",
    "        mind_violations_history.append(inconsistencies)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[MIND] Epoch {epoch}/{epochs} - Loss: {loss.item():.4f}, Test Acc: {test_acc*100:.1f}%, Violations: {inconsistencies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, params.get('epochs', 50) + 1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs_range, baseline_acc_history, label='Baseline', marker='o')\n",
    "plt.plot(epochs_range, mind_acc_history, label='MIND', marker='s')\n",
    "plt.title('Test Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs_range, baseline_violations_history, label='Baseline', marker='o')\n",
    "plt.plot(epochs_range, mind_violations_history, label='MIND', marker='s')\n",
    "plt.title('Number of Symmetry Rule Violations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Violations (count)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_base_acc = baseline_acc_history[-1]\n",
    "final_mind_acc = mind_acc_history[-1]\n",
    "final_base_viol = baseline_violations_history[-1]\n",
    "final_mind_viol = mind_violations_history[-1]\n",
    "\n",
    "print(f\"Final Baseline Accuracy: {final_base_acc*100:.1f}%\")\n",
    "print(f\"Final MIND Accuracy: {final_mind_acc*100:.1f}%\")\n",
    "print(f\"Final Baseline Violations: {final_base_viol}\")\n",
    "print(f\"Final MIND Violations: {final_mind_viol}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-options-description",
   "metadata": {},
   "source": [
    "## Optional Advanced Options\n",
    "\n",
    "Below are some optional advanced options for further exploration:\n",
    "\n",
    "1. **Complex Logical Rules**: Extend the logic loss to enforce rules such as transitivity (e.g., if A is friends with B and B with C, then A should be friends with C).\n",
    "2. **Multi-Task Learning**: Integrate additional tasks or relations into the model.\n",
    "3. **Model Architecture Variations**: Experiment with different embedding sizes, hidden layers, or even try graph neural network architectures.\n",
    "4. **Advanced Visualization**: Use techniques like t-SNE or PCA to visualize learned embeddings and the impact of logic constraints.\n",
    "\n",
    "To activate these options, modify or add new cells to implement the desired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-options-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Advanced configuration example\n",
    "\n",
    "def advanced_model_config(embed_dim=32, hidden_dim=64):\n",
    "    \"\"\"Return a new FriendPredictor model with advanced configuration.\"\"\"\n",
    "    return FriendPredictor(num_entities, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "# Uncomment below to experiment with an advanced model configuration\n",
    "# model_advanced = advanced_model_config(embed_dim=32, hidden_dim=64)\n",
    "# print('Advanced model configured with embed_dim=32 and hidden_dim=64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-cell",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This updated notebook has demonstrated the **MIND** framework for neuro‑symbolic AI with an integrated parameters UI, a detailed step‑by‑step tutorial, and optional advanced options. \n",
    "\n",
    "By combining neural network predictions with symbolic logical constraints, the MIND framework provides a robust and interpretable approach to enforcing domain-specific rules. \n",
    "\n",
    "Feel free to experiment with hyperparameters, explore advanced model configurations, and extend the logic constraints to suit more complex applications. Happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MIND_NeuroSymbolic_Demo_Updated.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

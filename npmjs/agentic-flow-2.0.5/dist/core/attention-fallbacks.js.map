{"version":3,"file":"attention-fallbacks.js","sourceRoot":"","sources":["../../src/core/attention-fallbacks.ts"],"names":[],"mappings":"AAAA;;;;;GAKG;AASH;;;GAGG;AACH,MAAM,UAAU,yBAAyB,CACvC,KAAe,EACf,GAAa,EACb,KAAe,EACf,IAAe;IAEf,MAAM,EAAE,GAAG,KAAK,CAAC,MAAM,CAAC;IAExB,+CAA+C;IAC/C,IAAI,KAAK,GAAG,CAAC,CAAC;IACd,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC;QAC5B,KAAK,IAAI,KAAK,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC;IAC7B,CAAC;IACD,KAAK,IAAI,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;IAEvB,yBAAyB;IACzB,IAAI,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,EAAE,CAAC;QAC1B,KAAK,GAAG,CAAC,QAAQ,CAAC;IACpB,CAAC;IAED,iCAAiC;IACjC,MAAM,QAAQ,GAAG,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;IACjC,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,iCAAiC;IAE1D,iBAAiB;IACjB,MAAM,MAAM,GAAG,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;IAE1C,OAAO,EAAE,MAAM,EAAE,OAAO,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC;AACvC,CAAC;AAED;;;;GAIG;AACH,MAAM,OAAO,kBAAkB;IACrB,QAAQ,CAAS;IACjB,SAAS,CAAS;IAClB,OAAO,CAAS;IAChB,YAAY,CAAe;IAC3B,UAAU,CAAe;IACzB,YAAY,CAAe;IAC3B,aAAa,CAAa;IAElC,YAAY,MAAuB;QACjC,IAAI,CAAC,QAAQ,GAAG,MAAM,CAAC,QAAQ,IAAI,CAAC,CAAC;QACrC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAClC,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,QAAQ,CAAC,CAAC;QAE1D,8BAA8B;QAC9B,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAC7C,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAC3C,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAC7C,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,uBAAuB,EAAE,CAAC;IACtD,CAAC;IAEO,iBAAiB;QACvB,MAAM,OAAO,GAAiB,EAAE,CAAC;QACjC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,QAAQ,EAAE,CAAC,EAAE,EAAE,CAAC;YACvC,MAAM,WAAW,GAAe,EAAE,CAAC;YACnC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,EAAE,CAAC,EAAE,EAAE,CAAC;gBACtC,MAAM,GAAG,GAAa,EAAE,CAAC;gBACzB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,SAAS,EAAE,CAAC,EAAE,EAAE,CAAC;oBACxC,GAAG,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,MAAM,EAAE,GAAG,GAAG,CAAC,GAAG,GAAG,CAAC,CAAC;gBACxC,CAAC;gBACD,WAAW,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;YACxB,CAAC;YACD,OAAO,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;QAC5B,CAAC;QACD,OAAO,OAAO,CAAC;IACjB,CAAC;IAEO,uBAAuB;QAC7B,MAAM,OAAO,GAAe,EAAE,CAAC;QAC/B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,SAAS,EAAE,CAAC,EAAE,EAAE,CAAC;YACxC,MAAM,GAAG,GAAa,EAAE,CAAC;YACzB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,SAAS,EAAE,CAAC,EAAE,EAAE,CAAC;gBACxC,GAAG,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,MAAM,EAAE,GAAG,GAAG,CAAC,GAAG,GAAG,CAAC,CAAC;YACxC,CAAC;YACD,OAAO,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;QACpB,CAAC;QACD,OAAO,OAAO,CAAC;IACjB,CAAC;IAED,OAAO,CACL,KAAe,EACf,GAAa,EACb,KAAe,EACf,IAAe;QAEf,MAAM,WAAW,GAAe,EAAE,CAAC;QACnC,MAAM,UAAU,GAAe,EAAE,CAAC;QAElC,oBAAoB;QACpB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,QAAQ,EAAE,CAAC,EAAE,EAAE,CAAC;YACvC,4BAA4B;YAC5B,MAAM,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,KAAK,EAAE,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,CAAC;YACpD,MAAM,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,EAAE,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YAChD,MAAM,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,KAAK,EAAE,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,CAAC;YAEpD,0BAA0B;YAC1B,MAAM,EAAE,MAAM,EAAE,OAAO,EAAE,GAAG,yBAAyB,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;YAErE,WAAW,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;YACzB,UAAU,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;QAC3B,CAAC;QAED,oBAAoB;QACpB,MAAM,YAAY,GAAG,WAAW,CAAC,IAAI,EAAE,CAAC;QAExC,oBAAoB;QACpB,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,YAAY,EAAE,IAAI,CAAC,aAAa,CAAC,CAAC;QAE9D,OAAO,EAAE,MAAM,EAAE,gBAAgB,EAAE,UAAU,EAAE,CAAC;IAClD,CAAC;IAEO,OAAO,CAAC,KAAe,EAAE,OAAmB;QAClD,MAAM,MAAM,GAAa,EAAE,CAAC;QAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YACxC,IAAI,GAAG,GAAG,CAAC,CAAC;YACZ,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBACtC,GAAG,IAAI,KAAK,CAAC,CAAC,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YAClC,CAAC;YACD,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;QACnB,CAAC;QACD,OAAO,MAAM,CAAC;IAChB,CAAC;CACF;AAED;;;;;GAKG;AACH,MAAM,OAAO,cAAc;IACjB,SAAS,CAAS;IAClB,SAAS,CAAS;IAE1B,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAClC,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC,GAAG,CAAC,EAAE,EAAE,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,YAAY;IAC7D,CAAC;IAED,OAAO,CACL,KAAiB,EACjB,GAAe,EACf,KAAiB,EACjB,WAAmB,CAAC;QAEpB,MAAM,MAAM,GAAG,KAAK,CAAC,MAAM,CAAC;QAC5B,MAAM,OAAO,GAAG,IAAI,CAAC,SAAS,GAAG,QAAQ,CAAC;QAE1C,MAAM,MAAM,GAAe,EAAE,CAAC;QAC9B,MAAM,eAAe,GAAe,EAAE,CAAC;QAEvC,0CAA0C;QAC1C,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,IAAI,CAAC,SAAS,EAAE,CAAC;YAChD,MAAM,QAAQ,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,SAAS,EAAE,MAAM,CAAC,CAAC;YAEtD,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,QAAQ,EAAE,EAAE,EAAE,EAAE,CAAC;gBACrC,MAAM,MAAM,GAAa,EAAE,CAAC;gBAC5B,IAAI,QAAQ,GAAG,CAAC,QAAQ,CAAC;gBAEzB,0CAA0C;gBAC1C,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,MAAM,EAAE,EAAE,EAAE,EAAE,CAAC;oBACnC,IAAI,KAAK,GAAG,CAAC,CAAC;oBACd,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,EAAE,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;wBAC1C,KAAK,IAAI,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;oBACrC,CAAC;oBACD,KAAK,IAAI,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;oBAC5B,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;oBACnB,QAAQ,GAAG,IAAI,CAAC,GAAG,CAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;gBACvC,CAAC;gBAED,6BAA6B;gBAC7B,MAAM,SAAS,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC;gBAC1D,MAAM,MAAM,GAAG,SAAS,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC;gBACpD,MAAM,OAAO,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;gBAE/C,yBAAyB;gBACzB,MAAM,SAAS,GAAG,IAAI,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;gBACrD,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,MAAM,EAAE,EAAE,EAAE,EAAE,CAAC;oBACnC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,EAAE,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;wBAC1C,SAAS,CAAC,CAAC,CAAC,IAAI,OAAO,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;oBAC7C,CAAC;gBACH,CAAC;gBAED,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;gBACvB,eAAe,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;YAChC,CAAC;QACH,CAAC;QAED,OAAO,EAAE,MAAM,EAAE,eAAe,EAAE,CAAC;IACrC,CAAC;CACF;AAED;;;;GAIG;AACH,MAAM,OAAO,eAAe;IAClB,SAAS,CAAS;IAClB,UAAU,CAAwB;IAE1C,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAClC,kBAAkB;QAClB,IAAI,CAAC,UAAU,GAAG,CAAC,CAAS,EAAE,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IACjE,CAAC;IAED,OAAO,CACL,KAAiB,EACjB,GAAe,EACf,KAAiB;QAEjB,MAAM,MAAM,GAAG,KAAK,CAAC,MAAM,CAAC;QAC5B,MAAM,GAAG,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC;QAE5B,oBAAoB;QACpB,MAAM,WAAW,GAAG,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC;QAC3D,MAAM,SAAS,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC;QAEvD,6CAA6C;QAC7C,MAAM,GAAG,GAAe,KAAK,CAAC,IAAI,CAAC,EAAE,MAAM,EAAE,IAAI,CAAC,SAAS,EAAE,EAAE,GAAG,EAAE,CAClE,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CACnB,CAAC;QAEF,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YAChC,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,IAAI,CAAC,SAAS,EAAE,EAAE,EAAE,EAAE,CAAC;gBAC3C,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,GAAG,EAAE,EAAE,EAAE,EAAE,CAAC;oBAChC,GAAG,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,IAAI,SAAS,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBACjD,CAAC;YACH,CAAC;QACH,CAAC;QAED,oBAAoB;QACpB,MAAM,MAAM,GAAe,EAAE,CAAC;QAC9B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YAChC,MAAM,GAAG,GAAa,EAAE,CAAC;YACzB,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,GAAG,EAAE,EAAE,EAAE,EAAE,CAAC;gBAChC,IAAI,GAAG,GAAG,CAAC,CAAC;gBACZ,KAAK,IAAI,EAAE,GAAG,CAAC,EAAE,EAAE,GAAG,IAAI,CAAC,SAAS,EAAE,EAAE,EAAE,EAAE,CAAC;oBAC3C,GAAG,IAAI,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,GAAG,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC;gBAC1C,CAAC;gBACD,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;YAChB,CAAC;YAED,YAAY;YACZ,MAAM,OAAO,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC;YAC1D,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,OAAO,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QAClD,CAAC;QAED,OAAO,EAAE,MAAM,EAAE,CAAC;IACpB,CAAC;CACF;AAED;;;;GAIG;AACH,MAAM,OAAO,mBAAmB;IACtB,SAAS,CAAS;IAClB,SAAS,CAAS;IAE1B,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAClC,IAAI,CAAC,SAAS,GAAG,CAAC,GAAG,CAAC,CAAC,0BAA0B;IACnD,CAAC;IAED,OAAO,CACL,KAAe,EACf,GAAa,EACb,KAAe;QAEf,mCAAmC;QACnC,MAAM,QAAQ,GAAG,IAAI,CAAC,kBAAkB,CAAC,KAAK,EAAE,GAAG,CAAC,CAAC;QAErD,gDAAgD;QAChD,MAAM,MAAM,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,QAAQ,CAAC,CAAC;QAEnC,iBAAiB;QACjB,MAAM,MAAM,GAAG,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;QAE1C,OAAO,EAAE,MAAM,EAAE,QAAQ,EAAE,CAAC;IAC9B,CAAC;IAEO,kBAAkB,CAAC,CAAW,EAAE,CAAW;QACjD,kDAAkD;QAClD,IAAI,UAAU,GAAG,CAAC,CAAC;QACnB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YAClC,MAAM,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC;YACzB,UAAU,IAAI,IAAI,GAAG,IAAI,CAAC;QAC5B,CAAC;QAED,MAAM,OAAO,GAAG,CAAC,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,CAAC,EAAE,EAAE,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC;QACrD,MAAM,OAAO,GAAG,CAAC,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,CAAC,EAAE,EAAE,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC;QAErD,MAAM,SAAS,GAAG,UAAU,CAAC;QAC7B,MAAM,WAAW,GAAG,CAAC,CAAC,GAAG,OAAO,CAAC,GAAG,CAAC,CAAC,GAAG,OAAO,CAAC,CAAC;QAElD,OAAO,IAAI,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,SAAS,CAAC,GAAG,WAAW,CAAC,CAAC;IACvD,CAAC;CACF;AAED;;;;GAIG;AACH,MAAM,OAAO,YAAY;IACf,OAAO,CAAuB;IAC9B,UAAU,CAAS;IACnB,aAAa,CAAa;IAElC,YAAY,MAAiD;QAC3D,IAAI,CAAC,UAAU,GAAG,MAAM,CAAC,UAAU,IAAI,CAAC,CAAC;QACzC,IAAI,CAAC,OAAO,GAAG,KAAK,CAAC,IAAI,CACvB,EAAE,MAAM,EAAE,IAAI,CAAC,UAAU,EAAE,EAC3B,GAAG,EAAE,CAAC,IAAI,kBAAkB,CAAC,MAAM,CAAC,CACrC,CAAC;QAEF,oCAAoC;QACpC,IAAI,CAAC,aAAa,GAAG,KAAK,CAAC,IAAI,CAAC,EAAE,MAAM,EAAE,IAAI,CAAC,UAAU,EAAE,EAAE,GAAG,EAAE,CAChE,KAAK,CAAC,IAAI,CAAC,EAAE,MAAM,EAAE,MAAM,CAAC,SAAS,EAAE,EAAE,GAAG,EAAE,CAAC,CAAC,IAAI,CAAC,MAAM,EAAE,GAAG,GAAG,CAAC,GAAG,GAAG,CAAC,CAC5E,CAAC;IACJ,CAAC;IAED,OAAO,CACL,KAAe,EACf,GAAa,EACb,KAAe,EACf,OAAe,CAAC;QAEhB,wBAAwB;QACxB,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;YACpD,IAAI,KAAK,GAAG,CAAC,CAAC;YACd,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBACtC,KAAK,IAAI,KAAK,CAAC,CAAC,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;YACjC,CAAC;YACD,OAAO,KAAK,CAAC;QACf,CAAC,CAAC,CAAC;QAEH,6BAA6B;QAC7B,MAAM,SAAS,GAAG,YAAY,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC;QACrD,MAAM,MAAM,GAAG,SAAS,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,CAAC;QACpD,MAAM,aAAa,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;QAErD,oBAAoB;QACpB,MAAM,aAAa,GAAG,aAAa;aAChC,GAAG,CAAC,CAAC,MAAM,EAAE,GAAG,EAAE,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,GAAG,EAAE,CAAC,CAAC;aACvC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,MAAM,GAAG,CAAC,CAAC,MAAM,CAAC;aACnC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;QAElB,yCAAyC;QACzC,MAAM,MAAM,GAAG,IAAI,KAAK,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;QAE/C,KAAK,MAAM,EAAE,MAAM,EAAE,GAAG,EAAE,IAAI,aAAa,EAAE,CAAC;YAC5C,MAAM,YAAY,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,KAAK,EAAE,GAAG,EAAE,KAAK,CAAC,CAAC,MAAM,CAAC;YACzE,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBACvC,MAAM,CAAC,CAAC,CAAC,IAAI,MAAM,GAAG,YAAY,CAAC,CAAC,CAAC,CAAC;YACxC,CAAC;QACH,CAAC;QAED,OAAO,EAAE,MAAM,EAAE,aAAa,EAAE,CAAC;IACnC,CAAC;CACF;AAED;;GAEG;AACH,MAAM,UAAU,0BAA0B;IACxC,IAAI,CAAC;QACH,MAAM,SAAS,GAAG,OAAO,CAAC,qBAAqB,CAAC,CAAC;QACjD,yBAAyB;QACzB,MAAM,MAAM,GAAG,SAAS,CAAC,cAAc,CACrC,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EACxB,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EACxB,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EACxB,CAAC,CACF,CAAC;QACF,OAAO,IAAI,CAAC;IACd,CAAC;IAAC,MAAM,CAAC;QACP,OAAO,KAAK,CAAC;IACf,CAAC;AACH,CAAC;AAED;;GAEG;AACH,MAAM,UAAU,eAAe,CAC7B,IAA8D,EAC9D,MAAuB;IAEvB,QAAQ,IAAI,EAAE,CAAC;QACb,KAAK,YAAY;YACf,OAAO,IAAI,kBAAkB,CAAC,MAAM,CAAC,CAAC;QACxC,KAAK,OAAO;YACV,OAAO,IAAI,cAAc,CAAC,MAAM,CAAC,CAAC;QACpC,KAAK,QAAQ;YACX,OAAO,IAAI,eAAe,CAAC,MAAM,CAAC,CAAC;QACrC,KAAK,YAAY;YACf,OAAO,IAAI,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACzC,KAAK,KAAK;YACR,OAAO,IAAI,YAAY,CAAC,MAAM,CAAC,CAAC;QAClC;YACE,OAAO,IAAI,kBAAkB,CAAC,MAAM,CAAC,CAAC;IAC1C,CAAC;AACH,CAAC","sourcesContent":["/**\n * Attention Module Fallbacks\n *\n * Since @ruvector/attention is completely broken, provide JavaScript fallbacks\n * Performance will be slower but functionality will work\n */\n\nexport interface AttentionConfig {\n  hiddenDim: number;\n  numHeads?: number;\n  dropoutRate?: number;\n  useFlash?: boolean;\n}\n\n/**\n * Scaled Dot-Product Attention\n * The core attention mechanism\n */\nexport function scaledDotProductAttention(\n  query: number[],\n  key: number[],\n  value: number[],\n  mask?: number[]\n): { output: number[]; weights: number[] } {\n  const dk = query.length;\n\n  // Compute attention scores: Q · K^T / sqrt(dk)\n  let score = 0;\n  for (let i = 0; i < dk; i++) {\n    score += query[i] * key[i];\n  }\n  score /= Math.sqrt(dk);\n\n  // Apply mask if provided\n  if (mask && mask[0] === 0) {\n    score = -Infinity;\n  }\n\n  // Softmax (single score version)\n  const expScore = Math.exp(score);\n  const weight = expScore; // Simplified for single K,V pair\n\n  // Weighted value\n  const output = value.map(v => v * weight);\n\n  return { output, weights: [weight] };\n}\n\n/**\n * Multi-Head Attention (JavaScript fallback)\n *\n * Replaces broken @ruvector/attention.multiHeadAttention\n */\nexport class MultiHeadAttention {\n  private numHeads: number;\n  private hiddenDim: number;\n  private headDim: number;\n  private queryWeights: number[][][];\n  private keyWeights: number[][][];\n  private valueWeights: number[][][];\n  private outputWeights: number[][];\n\n  constructor(config: AttentionConfig) {\n    this.numHeads = config.numHeads || 8;\n    this.hiddenDim = config.hiddenDim;\n    this.headDim = Math.floor(this.hiddenDim / this.numHeads);\n\n    // Initialize weights (random)\n    this.queryWeights = this.initializeWeights();\n    this.keyWeights = this.initializeWeights();\n    this.valueWeights = this.initializeWeights();\n    this.outputWeights = this.initializeOutputWeights();\n  }\n\n  private initializeWeights(): number[][][] {\n    const weights: number[][][] = [];\n    for (let h = 0; h < this.numHeads; h++) {\n      const headWeights: number[][] = [];\n      for (let i = 0; i < this.headDim; i++) {\n        const row: number[] = [];\n        for (let j = 0; j < this.hiddenDim; j++) {\n          row.push((Math.random() - 0.5) * 0.1);\n        }\n        headWeights.push(row);\n      }\n      weights.push(headWeights);\n    }\n    return weights;\n  }\n\n  private initializeOutputWeights(): number[][] {\n    const weights: number[][] = [];\n    for (let i = 0; i < this.hiddenDim; i++) {\n      const row: number[] = [];\n      for (let j = 0; j < this.hiddenDim; j++) {\n        row.push((Math.random() - 0.5) * 0.1);\n      }\n      weights.push(row);\n    }\n    return weights;\n  }\n\n  forward(\n    query: number[],\n    key: number[],\n    value: number[],\n    mask?: number[]\n  ): { output: number[]; attentionWeights: number[][] } {\n    const headOutputs: number[][] = [];\n    const allWeights: number[][] = [];\n\n    // Process each head\n    for (let h = 0; h < this.numHeads; h++) {\n      // Project to head dimension\n      const q = this.project(query, this.queryWeights[h]);\n      const k = this.project(key, this.keyWeights[h]);\n      const v = this.project(value, this.valueWeights[h]);\n\n      // Attention for this head\n      const { output, weights } = scaledDotProductAttention(q, k, v, mask);\n\n      headOutputs.push(output);\n      allWeights.push(weights);\n    }\n\n    // Concatenate heads\n    const concatenated = headOutputs.flat();\n\n    // Output projection\n    const output = this.project(concatenated, this.outputWeights);\n\n    return { output, attentionWeights: allWeights };\n  }\n\n  private project(input: number[], weights: number[][]): number[] {\n    const output: number[] = [];\n    for (let i = 0; i < weights.length; i++) {\n      let sum = 0;\n      for (let j = 0; j < input.length; j++) {\n        sum += input[j] * weights[i][j];\n      }\n      output.push(sum);\n    }\n    return output;\n  }\n}\n\n/**\n * Flash Attention (optimized fallback)\n *\n * Replaces broken @ruvector/attention.flashAttention\n * Uses tiling/chunking for better memory efficiency\n */\nexport class FlashAttention {\n  private hiddenDim: number;\n  private blockSize: number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n    this.blockSize = Math.min(64, this.hiddenDim); // Tile size\n  }\n\n  forward(\n    query: number[][],\n    key: number[][],\n    value: number[][],\n    numHeads: number = 8\n  ): { output: number[][]; attentionScores: number[][] } {\n    const seqLen = query.length;\n    const headDim = this.hiddenDim / numHeads;\n\n    const output: number[][] = [];\n    const attentionScores: number[][] = [];\n\n    // Process in blocks for memory efficiency\n    for (let i = 0; i < seqLen; i += this.blockSize) {\n      const blockEnd = Math.min(i + this.blockSize, seqLen);\n\n      for (let qi = i; qi < blockEnd; qi++) {\n        const scores: number[] = [];\n        let maxScore = -Infinity;\n\n        // Compute attention scores for this query\n        for (let ki = 0; ki < seqLen; ki++) {\n          let score = 0;\n          for (let d = 0; d < query[qi].length; d++) {\n            score += query[qi][d] * key[ki][d];\n          }\n          score /= Math.sqrt(headDim);\n          scores.push(score);\n          maxScore = Math.max(maxScore, score);\n        }\n\n        // Numerically stable softmax\n        const expScores = scores.map(s => Math.exp(s - maxScore));\n        const sumExp = expScores.reduce((a, b) => a + b, 0);\n        const weights = expScores.map(e => e / sumExp);\n\n        // Weighted sum of values\n        const outputRow = new Array(value[0].length).fill(0);\n        for (let vi = 0; vi < seqLen; vi++) {\n          for (let d = 0; d < value[vi].length; d++) {\n            outputRow[d] += weights[vi] * value[vi][d];\n          }\n        }\n\n        output.push(outputRow);\n        attentionScores.push(weights);\n      }\n    }\n\n    return { output, attentionScores };\n  }\n}\n\n/**\n * Linear Attention (fallback)\n *\n * O(n) complexity approximation of attention\n */\nexport class LinearAttention {\n  private hiddenDim: number;\n  private featureMap: (x: number) => number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n    // ELU feature map\n    this.featureMap = (x: number) => (x > 0 ? x : Math.exp(x) - 1);\n  }\n\n  forward(\n    query: number[][],\n    key: number[][],\n    value: number[][]\n  ): { output: number[][] } {\n    const seqLen = query.length;\n    const dim = value[0].length;\n\n    // Apply feature map\n    const queryMapped = query.map(q => q.map(this.featureMap));\n    const keyMapped = key.map(k => k.map(this.featureMap));\n\n    // Compute K^T V (dimension: [dim, valueDim])\n    const ktv: number[][] = Array.from({ length: this.hiddenDim }, () =>\n      Array(dim).fill(0)\n    );\n\n    for (let i = 0; i < seqLen; i++) {\n      for (let d1 = 0; d1 < this.hiddenDim; d1++) {\n        for (let d2 = 0; d2 < dim; d2++) {\n          ktv[d1][d2] += keyMapped[i][d1] * value[i][d2];\n        }\n      }\n    }\n\n    // Compute Q (K^T V)\n    const output: number[][] = [];\n    for (let i = 0; i < seqLen; i++) {\n      const row: number[] = [];\n      for (let d2 = 0; d2 < dim; d2++) {\n        let sum = 0;\n        for (let d1 = 0; d1 < this.hiddenDim; d1++) {\n          sum += queryMapped[i][d1] * ktv[d1][d2];\n        }\n        row.push(sum);\n      }\n\n      // Normalize\n      const normSum = queryMapped[i].reduce((a, b) => a + b, 0);\n      output.push(row.map(v => v / (normSum + 1e-8)));\n    }\n\n    return { output };\n  }\n}\n\n/**\n * Hyperbolic Attention (simplified fallback)\n *\n * Approximation using hyperbolic geometry\n */\nexport class HyperbolicAttention {\n  private hiddenDim: number;\n  private curvature: number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n    this.curvature = -1.0; // Poincaré ball curvature\n  }\n\n  forward(\n    query: number[],\n    key: number[],\n    value: number[]\n  ): { output: number[]; distance: number } {\n    // Hyperbolic distance (simplified)\n    const distance = this.hyperbolicDistance(query, key);\n\n    // Attention weight based on hyperbolic distance\n    const weight = Math.exp(-distance);\n\n    // Weighted value\n    const output = value.map(v => v * weight);\n\n    return { output, distance };\n  }\n\n  private hyperbolicDistance(a: number[], b: number[]): number {\n    // Simplified hyperbolic distance in Poincaré ball\n    let normDiffSq = 0;\n    for (let i = 0; i < a.length; i++) {\n      const diff = a[i] - b[i];\n      normDiffSq += diff * diff;\n    }\n\n    const normASq = a.reduce((sum, v) => sum + v * v, 0);\n    const normBSq = b.reduce((sum, v) => sum + v * v, 0);\n\n    const numerator = normDiffSq;\n    const denominator = (1 - normASq) * (1 - normBSq);\n\n    return Math.acosh(1 + (2 * numerator) / denominator);\n  }\n}\n\n/**\n * MoE (Mixture of Experts) Attention (fallback)\n *\n * Routes to different expert attention modules\n */\nexport class MoEAttention {\n  private experts: MultiHeadAttention[];\n  private numExperts: number;\n  private gatingWeights: number[][];\n\n  constructor(config: AttentionConfig & { numExperts?: number }) {\n    this.numExperts = config.numExperts || 4;\n    this.experts = Array.from(\n      { length: this.numExperts },\n      () => new MultiHeadAttention(config)\n    );\n\n    // Initialize gating network weights\n    this.gatingWeights = Array.from({ length: this.numExperts }, () =>\n      Array.from({ length: config.hiddenDim }, () => (Math.random() - 0.5) * 0.1)\n    );\n  }\n\n  forward(\n    query: number[],\n    key: number[],\n    value: number[],\n    topK: number = 2\n  ): { output: number[]; expertWeights: number[] } {\n    // Compute gating scores\n    const gatingScores = this.gatingWeights.map(weights => {\n      let score = 0;\n      for (let i = 0; i < query.length; i++) {\n        score += query[i] * weights[i];\n      }\n      return score;\n    });\n\n    // Softmax over top-K experts\n    const expScores = gatingScores.map(s => Math.exp(s));\n    const sumExp = expScores.reduce((a, b) => a + b, 0);\n    const expertWeights = expScores.map(e => e / sumExp);\n\n    // Get top-K experts\n    const expertIndices = expertWeights\n      .map((weight, idx) => ({ weight, idx }))\n      .sort((a, b) => b.weight - a.weight)\n      .slice(0, topK);\n\n    // Weighted combination of expert outputs\n    const output = new Array(query.length).fill(0);\n\n    for (const { weight, idx } of expertIndices) {\n      const expertOutput = this.experts[idx].forward(query, key, value).output;\n      for (let i = 0; i < output.length; i++) {\n        output[i] += weight * expertOutput[i];\n      }\n    }\n\n    return { output, expertWeights };\n  }\n}\n\n/**\n * Check if native attention is available\n */\nexport function isNativeAttentionAvailable(): boolean {\n  try {\n    const attention = require('@ruvector/attention');\n    // Try a simple operation\n    const result = attention.flashAttention(\n      new Float32Array([1, 0]),\n      new Float32Array([1, 0]),\n      new Float32Array([1, 0]),\n      1\n    );\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Factory function to create appropriate attention module\n */\nexport function createAttention(\n  type: 'multi-head' | 'flash' | 'linear' | 'hyperbolic' | 'moe',\n  config: AttentionConfig\n): MultiHeadAttention | FlashAttention | LinearAttention | HyperbolicAttention | MoEAttention {\n  switch (type) {\n    case 'multi-head':\n      return new MultiHeadAttention(config);\n    case 'flash':\n      return new FlashAttention(config);\n    case 'linear':\n      return new LinearAttention(config);\n    case 'hyperbolic':\n      return new HyperbolicAttention(config);\n    case 'moe':\n      return new MoEAttention(config);\n    default:\n      return new MultiHeadAttention(config);\n  }\n}\n"]}
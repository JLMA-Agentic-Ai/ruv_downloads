{"version":3,"file":"attention-native.js","sourceRoot":"","sources":["../../src/core/attention-native.ts"],"names":[],"mappings":"AAAA;;;;;GAKG;AAEH,OAAO,KAAK,eAAe,MAAM,qBAAqB,CAAC;AASvD;;GAEG;AACH,SAAS,cAAc,CAAC,KAA8B;IACpD,IAAI,KAAK,YAAY,YAAY,EAAE,CAAC;QAClC,OAAO,KAAK,CAAC;IACf,CAAC;IACD,OAAO,IAAI,YAAY,CAAC,KAAK,CAAC,CAAC;AACjC,CAAC;AAED;;GAEG;AACH,SAAS,OAAO,CAAC,KAAmB;IAClC,OAAO,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;AAC3B,CAAC;AAED;;;;;GAKG;AACH,MAAM,OAAO,kBAAkB;IACrB,cAAc,CAAM;IACpB,SAAS,CAAS;IAClB,QAAQ,CAAS;IAEzB,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAClC,IAAI,CAAC,QAAQ,GAAG,MAAM,CAAC,QAAQ,IAAI,CAAC,CAAC;QAErC,IAAI,CAAC;YACH,8BAA8B;YAC9B,IAAI,CAAC,cAAc,GAAG,IAAI,eAAe,CAAC,kBAAkB,CAC1D,IAAI,CAAC,SAAS,EACd,IAAI,CAAC,QAAQ,CACd,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,mDAAmD,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACtF,CAAC;IACH,CAAC;IAED;;OAEG;IACH,OAAO,CACL,KAA8B,EAC9B,GAA4B,EAC5B,KAA8B,EAC9B,IAA8B;QAE9B,IAAI,CAAC;YACH,0CAA0C;YAC1C,MAAM,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC;YAChC,MAAM,CAAC,GAAG,cAAc,CAAC,GAAG,CAAC,CAAC;YAC9B,MAAM,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC;YAChC,MAAM,CAAC,GAAG,IAAI,CAAC,CAAC,CAAC,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC;YAElD,6BAA6B;YAC7B,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;YAEvD,wCAAwC;YACxC,OAAO;gBACL,MAAM,EAAE,OAAO,CAAC,MAAM,CAAC;gBACvB,gBAAgB,EAAE,CAAC,EAAE,CAAC,CAAC,2CAA2C;aACnE,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,sCAAsC,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACzE,CAAC;IACH,CAAC;IAED;;OAEG;IACH,IAAI,GAAG;QACL,OAAO,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC;IACjC,CAAC;IAED,IAAI,OAAO;QACT,OAAO,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC;IACrC,CAAC;CACF;AAED;;;;GAIG;AACH,MAAM,OAAO,cAAc;IACjB,cAAc,CAAM;IACpB,SAAS,CAAS;IAE1B,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAElC,IAAI,CAAC;YACH,IAAI,CAAC,cAAc,GAAG,IAAI,eAAe,CAAC,cAAc,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;QAC3E,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,+CAA+C,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QAClF,CAAC;IACH,CAAC;IAED;;OAEG;IACH,OAAO,CACL,KAAkC,EAClC,GAAgC,EAChC,KAAkC,EAClC,WAAmB,CAAC;QAEpB,IAAI,CAAC;YACH,gCAAgC;YAChC,MAAM,CAAC,GAAG,KAAK,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;YACpC,MAAM,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;YAClC,MAAM,CAAC,GAAG,KAAK,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;YAEpC,sBAAsB;YACtB,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,QAAQ,CAAC,CAAC;YAE9D,sBAAsB;YACtB,OAAO;gBACL,MAAM,EAAE,MAAM,CAAC,GAAG,CAAC,CAAC,CAAe,EAAE,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;gBACnD,eAAe,EAAE,CAAC,EAAE,CAAC;aACtB,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,kCAAkC,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACrE,CAAC;IACH,CAAC;CACF;AAED;;;;GAIG;AACH,MAAM,OAAO,eAAe;IAClB,cAAc,CAAM;IACpB,SAAS,CAAS;IAE1B,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAElC,IAAI,CAAC;YACH,mDAAmD;YACnD,oEAAoE;YACpE,IAAI,CAAC,cAAc,GAAG,IAAI,eAAe,CAAC,eAAe,CACvD,IAAI,CAAC,SAAS,EACd,IAAI,CAAC,SAAS,CACf,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,gDAAgD,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACnF,CAAC;IACH,CAAC;IAED,OAAO,CACL,KAAkC,EAClC,GAAgC,EAChC,KAAkC;QAElC,IAAI,CAAC;YACH,MAAM,CAAC,GAAG,KAAK,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;YACpC,MAAM,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;YAClC,MAAM,CAAC,GAAG,KAAK,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;YAEpC,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;YAEpD,OAAO;gBACL,MAAM,EAAE,MAAM,CAAC,GAAG,CAAC,CAAC,CAAe,EAAE,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;aACpD,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,mCAAmC,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACtE,CAAC;IACH,CAAC;CACF;AAED;;GAEG;AACH,MAAM,OAAO,mBAAmB;IACtB,cAAc,CAAM;IACpB,SAAS,CAAS;IAE1B,YAAY,MAAuB;QACjC,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAElC,IAAI,CAAC;YACH,IAAI,CAAC,cAAc,GAAG,IAAI,eAAe,CAAC,mBAAmB,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;QAChF,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,oDAAoD,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACvF,CAAC;IACH,CAAC;IAED,OAAO,CACL,KAA8B,EAC9B,GAA4B,EAC5B,KAA8B;QAE9B,IAAI,CAAC;YACH,MAAM,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC;YAChC,MAAM,CAAC,GAAG,cAAc,CAAC,GAAG,CAAC,CAAC;YAC9B,MAAM,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC;YAEhC,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;YAEpD,OAAO;gBACL,MAAM,EAAE,OAAO,CAAC,MAAM,CAAC,MAAM,CAAC;gBAC9B,QAAQ,EAAE,MAAM,CAAC,QAAQ;aAC1B,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,uCAAuC,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QAC1E,CAAC;IACH,CAAC;CACF;AAED;;;;GAIG;AACH,MAAM,OAAO,YAAY;IACf,cAAc,CAAM;IACpB,SAAS,CAAS;IAClB,UAAU,CAAS;IAE3B,YAAY,MAAiD;QAC3D,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;QAClC,IAAI,CAAC,UAAU,GAAG,MAAM,CAAC,UAAU,IAAI,CAAC,CAAC;QAEzC,IAAI,CAAC;YACH,IAAI,CAAC,cAAc,GAAG,IAAI,eAAe,CAAC,YAAY,CACpD,IAAI,CAAC,SAAS,EACd,IAAI,CAAC,UAAU,CAChB,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,6CAA6C,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QAChF,CAAC;IACH,CAAC;IAED,OAAO,CACL,KAA8B,EAC9B,GAA4B,EAC5B,KAA8B,EAC9B,OAAe,CAAC;QAEhB,IAAI,CAAC;YACH,MAAM,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC;YAChC,MAAM,CAAC,GAAG,cAAc,CAAC,GAAG,CAAC,CAAC;YAC9B,MAAM,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC;YAEhC,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;YAE1D,OAAO;gBACL,MAAM,EAAE,OAAO,CAAC,MAAM,CAAC,MAAM,CAAC;gBAC9B,aAAa,EAAE,OAAO,CAAC,MAAM,CAAC,aAAa,CAAC;aAC7C,CAAC;QACJ,CAAC;QAAC,OAAO,KAAU,EAAE,CAAC;YACpB,MAAM,IAAI,KAAK,CAAC,gCAAgC,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;QACnE,CAAC;IACH,CAAC;CACF;AAED;;GAEG;AACH,MAAM,UAAU,yBAAyB,CACvC,KAAe,EACf,GAAa,EACb,KAAe,EACf,IAAe;IAEf,MAAM,EAAE,GAAG,KAAK,CAAC,MAAM,CAAC;IAExB,+CAA+C;IAC/C,IAAI,KAAK,GAAG,CAAC,CAAC;IACd,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC;QAC5B,KAAK,IAAI,KAAK,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC;IAC7B,CAAC;IACD,KAAK,IAAI,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;IAEvB,yBAAyB;IACzB,IAAI,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,EAAE,CAAC;QAC1B,KAAK,GAAG,CAAC,QAAQ,CAAC;IACpB,CAAC;IAED,iCAAiC;IACjC,MAAM,QAAQ,GAAG,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;IACjC,MAAM,MAAM,GAAG,QAAQ,CAAC;IAExB,iBAAiB;IACjB,MAAM,MAAM,GAAG,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;IAE1C,OAAO,EAAE,MAAM,EAAE,OAAO,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC;AACvC,CAAC;AAED;;GAEG;AACH,MAAM,UAAU,0BAA0B;IACxC,IAAI,CAAC;QACH,MAAM,IAAI,GAAG,IAAI,eAAe,CAAC,kBAAkB,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC;QAC5D,OAAO,IAAI,CAAC;IACd,CAAC;IAAC,MAAM,CAAC;QACP,OAAO,KAAK,CAAC;IACf,CAAC;AACH,CAAC;AAED;;GAEG;AACH,MAAM,UAAU,eAAe,CAC7B,IAA8D,EAC9D,MAAuB;IAEvB,QAAQ,IAAI,EAAE,CAAC;QACb,KAAK,YAAY;YACf,OAAO,IAAI,kBAAkB,CAAC,MAAM,CAAC,CAAC;QACxC,KAAK,OAAO;YACV,OAAO,IAAI,cAAc,CAAC,MAAM,CAAC,CAAC;QACpC,KAAK,QAAQ;YACX,OAAO,IAAI,eAAe,CAAC,MAAM,CAAC,CAAC;QACrC,KAAK,YAAY;YACf,OAAO,IAAI,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACzC,KAAK,KAAK;YACR,OAAO,IAAI,YAAY,CAAC,MAAM,CAAC,CAAC;QAClC;YACE,OAAO,IAAI,kBAAkB,CAAC,MAAM,CAAC,CAAC;IAC1C,CAAC;AACH,CAAC","sourcesContent":["/**\n * Native Attention Wrappers\n *\n * Properly wraps @ruvector/attention native Rust implementations\n * with TypedArray conversions and proper error handling\n */\n\nimport * as nativeAttention from '@ruvector/attention';\n\nexport interface AttentionConfig {\n  hiddenDim: number;\n  numHeads?: number;\n  dropoutRate?: number;\n  useFlash?: boolean;\n}\n\n/**\n * Convert regular array to Float32Array if needed\n */\nfunction toFloat32Array(input: number[] | Float32Array): Float32Array {\n  if (input instanceof Float32Array) {\n    return input;\n  }\n  return new Float32Array(input);\n}\n\n/**\n * Convert Float32Array to regular array for JS compatibility\n */\nfunction toArray(input: Float32Array): number[] {\n  return Array.from(input);\n}\n\n/**\n * Native Multi-Head Attention (uses Rust implementation)\n *\n * This wrapper properly converts between JavaScript arrays and TypedArrays\n * required by the native Rust implementation.\n */\nexport class MultiHeadAttention {\n  private nativeInstance: any;\n  private hiddenDim: number;\n  private numHeads: number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n    this.numHeads = config.numHeads || 8;\n\n    try {\n      // Create native Rust instance\n      this.nativeInstance = new nativeAttention.MultiHeadAttention(\n        this.hiddenDim,\n        this.numHeads\n      );\n    } catch (error: any) {\n      throw new Error(`Failed to initialize native MultiHeadAttention: ${error.message}`);\n    }\n  }\n\n  /**\n   * Forward pass using native Rust implementation\n   */\n  forward(\n    query: number[] | Float32Array,\n    key: number[] | Float32Array,\n    value: number[] | Float32Array,\n    mask?: number[] | Float32Array\n  ): { output: number[]; attentionWeights: number[][] } {\n    try {\n      // Convert to Float32Array for native code\n      const q = toFloat32Array(query);\n      const k = toFloat32Array(key);\n      const v = toFloat32Array(value);\n      const m = mask ? toFloat32Array(mask) : undefined;\n\n      // Call native compute method\n      const result = this.nativeInstance.compute(q, k, v, m);\n\n      // Convert result back to regular arrays\n      return {\n        output: toArray(result),\n        attentionWeights: [[]] // Native doesn't return weights separately\n      };\n    } catch (error: any) {\n      throw new Error(`MultiHeadAttention forward failed: ${error.message}`);\n    }\n  }\n\n  /**\n   * Get dimensions\n   */\n  get dim(): number {\n    return this.nativeInstance.dim;\n  }\n\n  get headDim(): number {\n    return this.nativeInstance.headDim;\n  }\n}\n\n/**\n * Native Flash Attention (uses Rust implementation)\n *\n * Memory-efficient attention with tiling/chunking\n */\nexport class FlashAttention {\n  private nativeInstance: any;\n  private hiddenDim: number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n\n    try {\n      this.nativeInstance = new nativeAttention.FlashAttention(this.hiddenDim);\n    } catch (error: any) {\n      throw new Error(`Failed to initialize native FlashAttention: ${error.message}`);\n    }\n  }\n\n  /**\n   * Forward pass with batch support\n   */\n  forward(\n    query: number[][] | Float32Array[],\n    key: number[][] | Float32Array[],\n    value: number[][] | Float32Array[],\n    numHeads: number = 8\n  ): { output: number[][]; attentionScores: number[][] } {\n    try {\n      // Convert batch to Float32Array\n      const q = query.map(toFloat32Array);\n      const k = key.map(toFloat32Array);\n      const v = value.map(toFloat32Array);\n\n      // Call native compute\n      const result = this.nativeInstance.compute(q, k, v, numHeads);\n\n      // Convert result back\n      return {\n        output: result.map((r: Float32Array) => toArray(r)),\n        attentionScores: [[]]\n      };\n    } catch (error: any) {\n      throw new Error(`FlashAttention forward failed: ${error.message}`);\n    }\n  }\n}\n\n/**\n * Native Linear Attention (uses Rust implementation)\n *\n * O(n) complexity approximation of attention\n */\nexport class LinearAttention {\n  private nativeInstance: any;\n  private hiddenDim: number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n\n    try {\n      // LinearAttention constructor: (hiddenDim, seqLen)\n      // We'll use hiddenDim for both since seqLen varies per forward call\n      this.nativeInstance = new nativeAttention.LinearAttention(\n        this.hiddenDim,\n        this.hiddenDim\n      );\n    } catch (error: any) {\n      throw new Error(`Failed to initialize native LinearAttention: ${error.message}`);\n    }\n  }\n\n  forward(\n    query: number[][] | Float32Array[],\n    key: number[][] | Float32Array[],\n    value: number[][] | Float32Array[]\n  ): { output: number[][] } {\n    try {\n      const q = query.map(toFloat32Array);\n      const k = key.map(toFloat32Array);\n      const v = value.map(toFloat32Array);\n\n      const result = this.nativeInstance.compute(q, k, v);\n\n      return {\n        output: result.map((r: Float32Array) => toArray(r))\n      };\n    } catch (error: any) {\n      throw new Error(`LinearAttention forward failed: ${error.message}`);\n    }\n  }\n}\n\n/**\n * Native Hyperbolic Attention (uses Rust implementation)\n */\nexport class HyperbolicAttention {\n  private nativeInstance: any;\n  private hiddenDim: number;\n\n  constructor(config: AttentionConfig) {\n    this.hiddenDim = config.hiddenDim;\n\n    try {\n      this.nativeInstance = new nativeAttention.HyperbolicAttention(this.hiddenDim);\n    } catch (error: any) {\n      throw new Error(`Failed to initialize native HyperbolicAttention: ${error.message}`);\n    }\n  }\n\n  forward(\n    query: number[] | Float32Array,\n    key: number[] | Float32Array,\n    value: number[] | Float32Array\n  ): { output: number[]; distance: number } {\n    try {\n      const q = toFloat32Array(query);\n      const k = toFloat32Array(key);\n      const v = toFloat32Array(value);\n\n      const result = this.nativeInstance.compute(q, k, v);\n\n      return {\n        output: toArray(result.output),\n        distance: result.distance\n      };\n    } catch (error: any) {\n      throw new Error(`HyperbolicAttention forward failed: ${error.message}`);\n    }\n  }\n}\n\n/**\n * Native MoE Attention (uses Rust implementation)\n *\n * Mixture of Experts with top-k routing\n */\nexport class MoEAttention {\n  private nativeInstance: any;\n  private hiddenDim: number;\n  private numExperts: number;\n\n  constructor(config: AttentionConfig & { numExperts?: number }) {\n    this.hiddenDim = config.hiddenDim;\n    this.numExperts = config.numExperts || 4;\n\n    try {\n      this.nativeInstance = new nativeAttention.MoEAttention(\n        this.hiddenDim,\n        this.numExperts\n      );\n    } catch (error: any) {\n      throw new Error(`Failed to initialize native MoEAttention: ${error.message}`);\n    }\n  }\n\n  forward(\n    query: number[] | Float32Array,\n    key: number[] | Float32Array,\n    value: number[] | Float32Array,\n    topK: number = 2\n  ): { output: number[]; expertWeights: number[] } {\n    try {\n      const q = toFloat32Array(query);\n      const k = toFloat32Array(key);\n      const v = toFloat32Array(value);\n\n      const result = this.nativeInstance.compute(q, k, v, topK);\n\n      return {\n        output: toArray(result.output),\n        expertWeights: toArray(result.expertWeights)\n      };\n    } catch (error: any) {\n      throw new Error(`MoEAttention forward failed: ${error.message}`);\n    }\n  }\n}\n\n/**\n * Scaled Dot-Product Attention (pure JavaScript, always works)\n */\nexport function scaledDotProductAttention(\n  query: number[],\n  key: number[],\n  value: number[],\n  mask?: number[]\n): { output: number[]; weights: number[] } {\n  const dk = query.length;\n\n  // Compute attention scores: Q Â· K^T / sqrt(dk)\n  let score = 0;\n  for (let i = 0; i < dk; i++) {\n    score += query[i] * key[i];\n  }\n  score /= Math.sqrt(dk);\n\n  // Apply mask if provided\n  if (mask && mask[0] === 0) {\n    score = -Infinity;\n  }\n\n  // Softmax (single score version)\n  const expScore = Math.exp(score);\n  const weight = expScore;\n\n  // Weighted value\n  const output = value.map(v => v * weight);\n\n  return { output, weights: [weight] };\n}\n\n/**\n * Check if native attention is available\n */\nexport function isNativeAttentionAvailable(): boolean {\n  try {\n    const test = new nativeAttention.MultiHeadAttention(128, 4);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Factory function to create appropriate attention module\n */\nexport function createAttention(\n  type: 'multi-head' | 'flash' | 'linear' | 'hyperbolic' | 'moe',\n  config: AttentionConfig\n): MultiHeadAttention | FlashAttention | LinearAttention | HyperbolicAttention | MoEAttention {\n  switch (type) {\n    case 'multi-head':\n      return new MultiHeadAttention(config);\n    case 'flash':\n      return new FlashAttention(config);\n    case 'linear':\n      return new LinearAttention(config);\n    case 'hyperbolic':\n      return new HyperbolicAttention(config);\n    case 'moe':\n      return new MoEAttention(config);\n    default:\n      return new MultiHeadAttention(config);\n  }\n}\n"]}
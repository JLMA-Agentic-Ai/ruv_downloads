/**
 * @claude-flow/performance - Flash Attention Integration
 *
 * Integrates @ruvector/attention Flash Attention capabilities into V3 performance module.
 * Provides optimized attention mechanisms with 2.49x-7.47x speedup targets.
 *
 * Features:
 * - Flash Attention for memory-efficient processing
 * - Automatic runtime selection (NAPI/WASM/JS)
 * - Performance benchmarking and metrics
 * - Speedup tracking and validation
 */
import { FlashAttention, DotProductAttention, type BenchmarkResult as AttentionBenchmarkResult } from '@ruvector/attention';
export interface AttentionInput {
    query: Float32Array | number[];
    keys: Float32Array[] | number[][];
    values: Float32Array[] | number[][];
    dim?: number;
    blockSize?: number;
}
export interface AttentionOutput {
    result: Float32Array;
    runtime: 'napi' | 'wasm' | 'js';
    executionTimeMs: number;
    memoryUsageBytes?: number;
}
export interface BenchmarkResult {
    flashAttention: {
        averageTimeMs: number;
        opsPerSecond: number;
        memoryUsageBytes?: number;
    };
    baseline: {
        averageTimeMs: number;
        opsPerSecond: number;
        memoryUsageBytes?: number;
    };
    speedup: number;
    meetsTarget: boolean;
    timestamp: Date;
}
export interface PerformanceMetrics {
    totalOperations: number;
    averageSpeedup: number;
    peakSpeedup: number;
    averageExecutionTimeMs: number;
    totalMemorySavedBytes: number;
    successRate: number;
    baselineMemoryBytes: number;
    optimizedMemoryBytes: number;
    memorySavedBytes: number;
    memorySavedPercent: number;
    peakMemoryBytes: number;
}
export declare class FlashAttentionOptimizer {
    private readonly dim;
    private readonly blockSize;
    private flashAttention;
    private baselineAttention;
    private metrics;
    constructor(dim?: number, blockSize?: number);
    /**
     * Optimize attention computation using Flash Attention
     * @param input - Query, keys, and values for attention computation
     * @returns Optimized attention output with performance metrics
     */
    optimize(input: AttentionInput): AttentionOutput;
    /**
     * Benchmark Flash Attention vs baseline attention
     * @returns Comprehensive benchmark results with speedup metrics
     */
    benchmark(): BenchmarkResult;
    /**
     * Get current speedup factor from accumulated metrics
     * @returns Average speedup factor across all operations
     */
    getSpeedup(): number;
    /**
     * Get comprehensive performance metrics
     * @returns Detailed performance statistics
     */
    getMetrics(): PerformanceMetrics;
    /**
     * Reset all metrics
     */
    resetMetrics(): void;
    /**
     * Ensure input is Float32Array for optimal performance
     */
    private ensureFloat32Array;
    /**
     * Detect which runtime is being used
     */
    private detectRuntime;
    /**
     * Get current memory usage in bytes
     */
    private getMemoryUsage;
    /**
     * Force garbage collection if available (requires --expose-gc flag)
     * This helps get more accurate memory measurements
     */
    private forceGC;
    /**
     * Benchmark memory usage across multiple dimensions
     * Validates the 50-75% memory reduction target
     * @param dimensions - Array of dimensions to test (default: [128, 256, 512, 1024])
     * @returns Memory profiling results for each dimension
     */
    benchmarkMemory(dimensions?: number[]): {
        dimension: number;
        baselineMemoryBytes: number;
        optimizedMemoryBytes: number;
        memorySavedBytes: number;
        memorySavedPercent: number;
        meetsTarget: boolean;
    }[];
}
/**
 * Create a Flash Attention optimizer with default settings
 * @param dim - Dimension of attention vectors (default: 512)
 * @param blockSize - Block size for Flash Attention (default: 64)
 * @returns Configured FlashAttentionOptimizer instance
 */
export declare function createFlashAttentionOptimizer(dim?: number, blockSize?: number): FlashAttentionOptimizer;
/**
 * Quick benchmark of Flash Attention performance
 * @param dim - Dimension to test (default: 512)
 * @returns Benchmark results with speedup metrics
 */
export declare function quickBenchmark(dim?: number): BenchmarkResult;
export { FlashAttention, DotProductAttention, type AttentionBenchmarkResult, };
//# sourceMappingURL=attention-integration.d.ts.map
{
  "url": "https://api.github.com/gists/5eabfef63adddd272d756e42b131deb7",
  "forks_url": "https://api.github.com/gists/5eabfef63adddd272d756e42b131deb7/forks",
  "commits_url": "https://api.github.com/gists/5eabfef63adddd272d756e42b131deb7/commits",
  "id": "5eabfef63adddd272d756e42b131deb7",
  "node_id": "G_kwDOACzGetoAIDVlYWJmZWY2M2FkZGRkMjcyZDc1NmU0MmIxMzFkZWI3",
  "git_pull_url": "https://gist.github.com/5eabfef63adddd272d756e42b131deb7.git",
  "git_push_url": "https://gist.github.com/5eabfef63adddd272d756e42b131deb7.git",
  "html_url": "https://gist.github.com/ruvnet/5eabfef63adddd272d756e42b131deb7",
  "files": {
    "Notebook.ipynb": {
      "filename": "Notebook.ipynb",
      "type": "text/plain",
      "language": "Jupyter Notebook",
      "raw_url": "https://gist.githubusercontent.com/ruvnet/5eabfef63adddd272d756e42b131deb7/raw/a7f3c61fbb0b4f95e6e98e2c30f0b8e9f4869b80/Notebook.ipynb",
      "size": 18230,
      "truncated": false,
      "content": "{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"id\": \"3b71edb4\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Applying GRPO to DeepSeek-R1-Distill-Qwen-1.5B with LIMO Dataset\\n\",\n        \"\\n\",\n        \"This notebook provides a step-by-step tutorial for applying **Generalized Reinforcement Policy Optimization (GRPO)** to the distilled model **DeepSeek-R1-Distill-Qwen-1.5B** using the high-quality LIMO dataset. We will cover:\\n\",\n        \"\\n\",\n        \"1. **Setup & Installation** – Installing dependencies and verifying GPU availability.\\n\",\n        \"2. **Model & Dataset Preparation** – Loading the model, tokenizer, and dataset, and formatting prompts.\\n\",\n        \"3. **Reinforcement Learning Fine-Tuning (GRPO)** – Implementing a simplified GRPO training loop, including reward computation and KL regularization.\\n\",\n        \"4. **Evaluation & Performance Metrics** – Demonstrating how to evaluate the fine-tuned model on benchmark tasks.\\n\",\n        \"5. **Hyperparameter Ablations & Future Directions** – Discussion on tuning and potential improvements.\\n\",\n        \"\\n\",\n        \"Let's begin!\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"id\": \"b55f4e70\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Setup & Installation\\n\",\n        \"\\n\",\n        \"We first install the necessary libraries including PyTorch, Hugging Face Transformers, TRL (for reinforcement learning), the Datasets library, and bitsandbytes for 8-bit optimization. Then, we verify that a GPU is available.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"7f4e7711\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"!pip install transformers==4.48.2 trl==0.15.0.dev0 datasets bitsandbytes accelerate\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"c6b72a2c\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import torch\\n\",\n        \"print(\\\"Torch version:\\\", torch.__version__)\\n\",\n        \"if torch.cuda.is_available():\\n\",\n        \"    device_name = torch.cuda.get_device_name(0)\\n\",\n        \"    print(\\\"GPU detected:\\\", device_name)\\n\",\n        \"    # Enable TF32 for faster matrix multiplication on supported GPUs\\n\",\n        \"    torch.backends.cuda.matmul.allow_tf32 = True\\n\",\n        \"else:\\n\",\n        \"    print(\\\"No GPU found. Please enable a GPU runtime for training.\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"id\": \"eb314410\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Model & Dataset Preparation\\n\",\n        \"\\n\",\n        \"We now load the **DeepSeek-R1-Distill-Qwen-1.5B** model and its tokenizer from Hugging Face, and load the LIMO dataset. The dataset consists of high-quality reasoning samples with a `question`, a detailed `solution`, and the final `answer`.\\n\",\n        \"\\n\",\n        \"We also define a helper function `format_prompt` that formats the question into a prompt instructing the model to output a reasoning chain and final answer using the tags `<think>` and `<answer>`.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"e3f3f80e\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\",\n        \"\\n\",\n        \"model_name = \\\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\\\"\\n\",\n        \"\\n\",\n        \"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\\n\",\n        \"model = AutoModelForCausalLM.from_pretrained(\\n\",\n        \"    model_name, \\n\",\n        \"    torch_dtype=torch.float16, \\n\",\n        \"    device_map=\\\"auto\\\"\\n\",\n        \"    # Uncomment the following line if the model requires custom code\\n\",\n        \"    # trust_remote_code=True\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Quick test generation\\n\",\n        \"prompt_test = \\\"What is the capital of France?\\\"\\n\",\n        \"inputs_test = tokenizer(prompt_test, return_tensors=\\\"pt\\\").to(model.device)\\n\",\n        \"outputs_test = model.generate(**inputs_test, max_new_tokens=10)\\n\",\n        \"print(\\\"Test output:\\\", tokenizer.decode(outputs_test[0], skip_special_tokens=True))\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"2b8f4d2e\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"from datasets import load_dataset\\n\",\n        \"\\n\",\n        \"# Load the LIMO dataset\\n\",\n        \"dataset = load_dataset(\\\"GAIR/LIMO\\\")\\n\",\n        \"train_data = dataset[\\\"train\\\"]\\n\",\n        \"print(\\\"Total training samples:\\\", len(train_data))\\n\",\n        \"\\n\",\n        \"# Display a sample\\n\",\n        \"sample = train_data[0]\\n\",\n        \"print(\\\"Question:\\\", sample[\\\"question\\\"])\\n\",\n        \"print(\\\"Solution (excerpt):\\\", sample[\\\"solution\\\"][:100] + \\\"...\\\")\\n\",\n        \"print(\\\"Answer:\\\", sample[\\\"answer\\\"])\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"eaeb2a11\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def format_prompt(question):\\n\",\n        \"    \\\"\\\"\\\"\\n\",\n        \"    Format the prompt to instruct the model to output a chain-of-thought and final answer.\\n\",\n        \"    \\\"\\\"\\\"\\n\",\n        \"    instruction = (\\n\",\n        \"        \\\"Solve the following problem step by step, then give the final answer. \\\"\\n\",\n        \"        \\\"Format your response as: <think>[reasoning]</think><answer>[final answer]</answer>.\\\"\\n\",\n        \"    )\\n\",\n        \"    return f\\\"{instruction}\\\\nQuestion: {question}\\\\nSolution:\\\"\\n\",\n        \"\\n\",\n        \"# Test the formatting\\n\",\n        \"formatted_prompt = format_prompt(sample[\\\"question\\\"])\\n\",\n        \"print(formatted_prompt)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"id\": \"f0fce501\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Reinforcement Learning Fine-Tuning (GRPO)\\n\",\n        \"\\n\",\n        \"In this section, we implement a simplified GRPO training loop. The main steps include:\\n\",\n        \"\\n\",\n        \"- **Sampling:** For each prompt, we generate multiple outputs (a group) from the model.\\n\",\n        \"- **Reward Scoring:** Compute a reward for each output based on answer accuracy and proper formatting.\\n\",\n        \"- **Advantage Calculation:** Compute the advantage by comparing each reward to the group average.\\n\",\n        \"- **Policy Optimization:** Update the model weights using the advantage-weighted log-likelihood loss along with a KL divergence penalty to keep the model close to the reference (base) policy.\\n\",\n        \"\\n\",\n        \"We use a default learning rate of `1e-6`, group size of 7, and a KL weight `β = 0.04`. We also set up an optimizer that supports 8-bit parameters (via bitsandbytes) for memory efficiency.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"d0b7a87d\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import math\\n\",\n        \"from transformers import AdamW  # Standard AdamW\\n\",\n        \"\\n\",\n        \"# Hyperparameters\\n\",\n        \"learning_rate = 1e-6\\n\",\n        \"tokens_per_generation = 4096  # Maximum tokens per generation (can be ablated)\\n\",\n        \"group_size = 7\\n\",\n        \"beta = 0.04\\n\",\n        \"\\n\",\n        \"# Initialize the 8-bit AdamW optimizer (using bitsandbytes)\\n\",\n        \"import bitsandbytes as bnb\\n\",\n        \"optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate)\\n\",\n        \"\\n\",\n        \"# Optionally, use standard 32-bit AdamW:\\n\",\n        \"# optimizer = AdamW(model.parameters(), lr=learning_rate)\\n\",\n        \"\\n\",\n        \"# Clone the initial model to serve as the reference for KL divergence\\n\",\n        \"from transformers import AutoModelForCausalLM\\n\",\n        \"ref_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(model.device)\\n\",\n        \"ref_model.eval()\\n\",\n        \"for param in ref_model.parameters():\\n\",\n        \"    param.requires_grad = False\\n\",\n        \"\\n\",\n        \"def reward_function(question, generated_text, true_answer):\\n\",\n        \"    \\\"\\\"\\\"\\n\",\n        \"    A simple rule-based reward:\\n\",\n        \"      - +0.1 bonus if output contains both <think> and <answer> tags\\n\",\n        \"      - +1.0 if the extracted answer matches the true answer\\n\",\n        \"      - Small penalty if no answer is extracted\\n\",\n        \"    \\\"\\\"\\\"\\n\",\n        \"    answer = None\\n\",\n        \"    if \\\"<answer>\\\" in generated_text and \\\"</answer>\\\" in generated_text:\\n\",\n        \"        start = generated_text.index(\\\"<answer>\\\") + len(\\\"<answer>\\\")\\n\",\n        \"        end = generated_text.index(\\\"</answer>\\\")\\n\",\n        \"        answer = generated_text[start:end].strip()\\n\",\n        \"    else:\\n\",\n        \"        # Fallback: take the last token as the answer\\n\",\n        \"        answer = generated_text.strip().split()[-1]\\n\",\n        \"\\n\",\n        \"    reward = 0.0\\n\",\n        \"    # Bonus for proper formatting\\n\",\n        \"    if \\\"<think>\\\" in generated_text and \\\"</think>\\\" in generated_text and \\\"<answer>\\\" in generated_text and \\\"</answer>\\\" in generated_text:\\n\",\n        \"        reward += 0.1\\n\",\n        \"    \\n\",\n        \"    # Reward based on answer accuracy\\n\",\n        \"    if answer is not None:\\n\",\n        \"        pred_ans = answer.strip().strip('.')\\n\",\n        \"        true_ans = str(true_answer).strip().strip('.')\\n\",\n        \"        if pred_ans == true_ans:\\n\",\n        \"            reward += 1.0\\n\",\n        \"    else:\\n\",\n        \"        reward -= 0.1\\n\",\n        \"    \\n\",\n        \"    return reward\\n\",\n        \"\\n\",\n        \"print(\\\"Optimizer and reward function set up.\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"3c51b0ea\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import random\\n\",\n        \"\\n\",\n        \"model.train()\\n\",\n        \"max_train_steps = 2  # Demo steps; in practice, use many more steps\\n\",\n        \"grad_accum_steps = 8  # Effective batch: grad_accum_steps * group_size\\n\",\n        \"\\n\",\n        \"# Shuffle training indices\\n\",\n        \"indices = list(range(len(train_data)))\\n\",\n        \"random.shuffle(indices)\\n\",\n        \"\\n\",\n        \"step = 0\\n\",\n        \"optimizer.zero_grad()\\n\",\n        \"\\n\",\n        \"for idx in indices[: max_train_steps * grad_accum_steps]:\\n\",\n        \"    question = train_data[idx][\\\"question\\\"]\\n\",\n        \"    true_answer = train_data[idx][\\\"answer\\\"]\\n\",\n        \"    prompt = format_prompt(question)\\n\",\n        \"    input_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids.to(model.device)\\n\",\n        \"    \\n\",\n        \"    # Generate a group of outputs\\n\",\n        \"    generated_texts = []\\n\",\n        \"    for _ in range(group_size):\\n\",\n        \"        output_ids = model.generate(\\n\",\n        \"            input_ids, \\n\",\n        \"            max_new_tokens=200,  # For demo; in practice, use tokens_per_generation\\n\",\n        \"            do_sample=True, \\n\",\n        \"            temperature=1.0,\\n\",\n        \"            eos_token_id=tokenizer.convert_tokens_to_ids(\\\"</answer>\\\")\\n\",\n        \"        )\\n\",\n        \"        generated = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\\n\",\n        \"        generated_texts.append(generated)\\n\",\n        \"    \\n\",\n        \"    # Compute rewards and advantages\\n\",\n        \"    rewards = [reward_function(question, text, true_answer) for text in generated_texts]\\n\",\n        \"    baseline = sum(rewards) / len(rewards)\\n\",\n        \"    advantages = [r - baseline for r in rewards]\\n\",\n        \"    \\n\",\n        \"    # Compute policy loss\\n\",\n        \"    policy_loss = 0.0\\n\",\n        \"    for text, adv in zip(generated_texts, advantages):\\n\",\n        \"        full_text = prompt + text\\n\",\n        \"        enc = tokenizer(full_text, return_tensors=\\\"pt\\\").to(model.device)\\n\",\n        \"        labels = enc.input_ids.clone()\\n\",\n        \"        labels[:, :input_ids.shape[1]] = -100  # Mask prompt tokens from loss\\n\",\n        \"        out = model(**enc, labels=labels)\\n\",\n        \"        # Multiply the average loss by the number of output tokens\\n\",\n        \"        policy_loss += adv * (out.loss * labels[:, input_ids.shape[1]:].numel())\\n\",\n        \"    policy_loss = policy_loss / group_size\\n\",\n        \"    \\n\",\n        \"    # Approximate KL divergence loss\\n\",\n        \"    kl_loss = 0.0\\n\",\n        \"    for text in generated_texts:\\n\",\n        \"        full_text = prompt + text\\n\",\n        \"        enc = tokenizer(full_text, return_tensors=\\\"pt\\\").to(model.device)\\n\",\n        \"        labels = enc.input_ids.clone()\\n\",\n        \"        labels[:, :input_ids.shape[1]] = -100\\n\",\n        \"        with torch.no_grad():\\n\",\n        \"            curr_out = model(**enc, labels=labels)\\n\",\n        \"            ref_out = ref_model(**enc, labels=labels)\\n\",\n        \"        curr_nll = curr_out.loss * labels[:, input_ids.shape[1]:].numel()\\n\",\n        \"        ref_nll = ref_out.loss * labels[:, input_ids.shape[1]:].numel()\\n\",\n        \"        kl_loss += (curr_nll - ref_nll) / labels[:, input_ids.shape[1]:].numel()\\n\",\n        \"    kl_loss = kl_loss / group_size\\n\",\n        \"    \\n\",\n        \"    total_loss = policy_loss + beta * kl_loss\\n\",\n        \"    total_loss.backward()\\n\",\n        \"    \\n\",\n        \"    if (idx + 1) % grad_accum_steps == 0:\\n\",\n        \"        optimizer.step()\\n\",\n        \"        optimizer.zero_grad()\\n\",\n        \"        step += 1\\n\",\n        \"        print(f\\\"Step {step}: policy_loss={policy_loss.item():.4f}, kl_loss={kl_loss.item():.4f}, rewards={rewards}\\\")\\n\",\n        \"        if step >= max_train_steps:\\n\",\n        \"            break\\n\",\n        \"\\n\",\n        \"model.eval()\\n\",\n        \"print(\\\"Training demo completed.\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"id\": \"7e36ea3a\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Evaluation & Performance Metrics\\n\",\n        \"\\n\",\n        \"After fine-tuning, we evaluate the model on reasoning benchmarks (e.g., AIME24, GPQA, MATH-500). In this demo, we show an evaluation example for one benchmark. \\n\",\n        \"\\n\",\n        \"The process involves:\\n\",\n        \"\\n\",\n        \"- Formatting the prompt as during training.\\n\",\n        \"- Generating an answer using greedy decoding.\\n\",\n        \"- Extracting the answer using the `<answer>` tags and comparing it with the ground truth.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"id\": \"97d86ce3\",\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Example evaluation for a benchmark (e.g., AIME24)\\n\",\n        \"# For illustration, let's assume we have lists of questions and true answers\\n\",\n        \"\\n\",\n        \"aime_questions = [\\n\",\n        \"    \\\"If x + y = 10 and x - y = 2, what is the value of x?\\\",\\n\",\n        \"    \\\"Compute the area of a circle with radius 7.\\\"\\n\",\n        \"]\\n\",\n        \"aime_answers = [\\n\",\n        \"    \\\"6\\\",  # x = 6\\n\",\n        \"    \\\"153.938\\\"  # Approximate area (could be rounded)\\n\",\n        \"]\\n\",\n        \"\\n\",\n        \"model.eval()\\n\",\n        \"correct = 0\\n\",\n        \"for question, true_answer in zip(aime_questions, aime_answers):\\n\",\n        \"    prompt = format_prompt(question)\\n\",\n        \"    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(model.device)\\n\",\n        \"    output_ids = model.generate(**inputs, max_new_tokens=512, temperature=0.0)  # Greedy decoding\\n\",\n        \"    output_text = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\\n\",\n        \"    \\n\",\n        \"    if \\\"<answer>\\\" in output_text and \\\"</answer>\\\" in output_text:\\n\",\n        \"        ans = output_text.split(\\\"<answer>\\\")[1].split(\\\"</answer>\\\")[0].strip()\\n\",\n        \"    else:\\n\",\n        \"        ans = output_text.strip().split()[-1]\\n\",\n        \"    \\n\",\n        \"    print(f\\\"Question: {question}\\\")\\n\",\n        \"    print(f\\\"Predicted Answer: {ans}\\\")\\n\",\n        \"    print(f\\\"True Answer: {true_answer}\\\\n\\\")\\n\",\n        \"    \\n\",\n        \"    if str(ans).strip().strip('.') == str(true_answer).strip().strip('.'):\\n\",\n        \"        correct += 1\\n\",\n        \"\\n\",\n        \"accuracy = correct / len(aime_questions) * 100\\n\",\n        \"print(f\\\"AIME24 Accuracy: {accuracy:.1f}%\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"id\": \"d7e82731\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. Hyperparameter Ablations & Future Directions\\n\",\n        \"\\n\",\n        \"### Hyperparameter Ablations\\n\",\n        \"\\n\",\n        \"Key hyperparameters that can be tuned include:\\n\",\n        \"\\n\",\n        \"- **Learning Rate:** Our default is `1e-6`, but values like `2e-6`, `4e-6`, or `8e-6` may be experimented with.\\n\",\n        \"- **Group Size:** Number of outputs per prompt (default is 7). Increasing this (e.g., 14, 28, or 56) can provide a more robust reward baseline but at higher computational cost.\\n\",\n        \"- **KL Weight (β):** Default is `0.04`. Lower values (e.g., 0.01 or 0.001) allow the model more freedom to explore but may risk divergence.\\n\",\n        \"\\n\",\n        \"### Future Directions\\n\",\n        \"\\n\",\n        \"- **Refining the Reward Function:** Improve extraction of the final answer and consider partial rewards for nearly correct outputs.\\n\",\n        \"- **Adaptive KL Penalty:** Use adaptive techniques to adjust β based on the observed KL divergence during training.\\n\",\n        \"- **Scaling Up:** Experiment with larger models or longer generation tokens to fully exploit the reasoning capabilities.\\n\",\n        \"- **Distillation vs. Pretrained Models:** Compare training outcomes when starting from a distilled model versus a base pretrained model.\\n\",\n        \"\\n\",\n        \"This concludes our step-by-step guide. Happy fine-tuning!\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"colab\": {\n      \"name\": \"GRPO_FineTuning_DeepSeek-R1-Distill-Qwen-1.5B.ipynb\",\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\",\n      \"version\": \"3.x\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 5\n}\n",
      "encoding": "utf-8"
    }
  },
  "public": true,
  "created_at": "2025-02-09T15:11:52Z",
  "updated_at": "2025-11-07T13:45:22Z",
  "description": "Applying GRPO to DeepSeek-R1-Distill-Qwen-1.5B with LIMO Dataset",
  "comments": 0,
  "user": null,
  "comments_enabled": true,
  "comments_url": "https://api.github.com/gists/5eabfef63adddd272d756e42b131deb7/comments",
  "owner": {
    "login": "ruvnet",
    "id": 2934394,
    "node_id": "MDQ6VXNlcjI5MzQzOTQ=",
    "avatar_url": "https://avatars.githubusercontent.com/u/2934394?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ruvnet",
    "html_url": "https://github.com/ruvnet",
    "followers_url": "https://api.github.com/users/ruvnet/followers",
    "following_url": "https://api.github.com/users/ruvnet/following{/other_user}",
    "gists_url": "https://api.github.com/users/ruvnet/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ruvnet/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ruvnet/subscriptions",
    "organizations_url": "https://api.github.com/users/ruvnet/orgs",
    "repos_url": "https://api.github.com/users/ruvnet/repos",
    "events_url": "https://api.github.com/users/ruvnet/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ruvnet/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "forks": [
    {
      "url": "https://api.github.com/gists/a57d20c503b5eed576d8101cf59b07d3",
      "user": {
        "login": "danbri",
        "id": 170265,
        "node_id": "MDQ6VXNlcjE3MDI2NQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/170265?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/danbri",
        "html_url": "https://github.com/danbri",
        "followers_url": "https://api.github.com/users/danbri/followers",
        "following_url": "https://api.github.com/users/danbri/following{/other_user}",
        "gists_url": "https://api.github.com/users/danbri/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/danbri/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/danbri/subscriptions",
        "organizations_url": "https://api.github.com/users/danbri/orgs",
        "repos_url": "https://api.github.com/users/danbri/repos",
        "events_url": "https://api.github.com/users/danbri/events{/privacy}",
        "received_events_url": "https://api.github.com/users/danbri/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false,
        "name": "Dan Brickley",
        "company": null,
        "blog": "https://danbri.org/",
        "location": "London",
        "email": "danbri@danbri.org",
        "hireable": null,
        "bio": "Schema.org team, etc.",
        "twitter_username": "danbri",
        "public_repos": 63,
        "public_gists": 419,
        "followers": 0,
        "following": 0,
        "created_at": "2009-12-20T22:42:23Z",
        "updated_at": "2026-01-23T13:27:10Z"
      },
      "id": "a57d20c503b5eed576d8101cf59b07d3",
      "created_at": "2025-02-17T21:15:43Z",
      "updated_at": "2025-02-17T21:15:44Z"
    },
    {
      "url": "https://api.github.com/gists/c9f56163f2489544ed0b00ebac4c4762",
      "user": {
        "login": "n4s5ti",
        "id": 77896784,
        "node_id": "MDQ6VXNlcjc3ODk2Nzg0",
        "avatar_url": "https://avatars.githubusercontent.com/u/77896784?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/n4s5ti",
        "html_url": "https://github.com/n4s5ti",
        "followers_url": "https://api.github.com/users/n4s5ti/followers",
        "following_url": "https://api.github.com/users/n4s5ti/following{/other_user}",
        "gists_url": "https://api.github.com/users/n4s5ti/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/n4s5ti/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/n4s5ti/subscriptions",
        "organizations_url": "https://api.github.com/users/n4s5ti/orgs",
        "repos_url": "https://api.github.com/users/n4s5ti/repos",
        "events_url": "https://api.github.com/users/n4s5ti/events{/privacy}",
        "received_events_url": "https://api.github.com/users/n4s5ti/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false,
        "name": null,
        "company": null,
        "blog": "",
        "location": null,
        "email": null,
        "hireable": null,
        "bio": null,
        "twitter_username": null,
        "public_repos": 105,
        "public_gists": 103,
        "followers": 9,
        "following": 60,
        "created_at": "2021-01-23T18:37:16Z",
        "updated_at": "2026-01-20T22:26:03Z"
      },
      "id": "c9f56163f2489544ed0b00ebac4c4762",
      "created_at": "2025-11-07T13:45:22Z",
      "updated_at": "2025-11-07T13:45:22Z"
    }
  ],
  "history": [
    {
      "user": {
        "login": "ruvnet",
        "id": 2934394,
        "node_id": "MDQ6VXNlcjI5MzQzOTQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2934394?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ruvnet",
        "html_url": "https://github.com/ruvnet",
        "followers_url": "https://api.github.com/users/ruvnet/followers",
        "following_url": "https://api.github.com/users/ruvnet/following{/other_user}",
        "gists_url": "https://api.github.com/users/ruvnet/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ruvnet/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ruvnet/subscriptions",
        "organizations_url": "https://api.github.com/users/ruvnet/orgs",
        "repos_url": "https://api.github.com/users/ruvnet/repos",
        "events_url": "https://api.github.com/users/ruvnet/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ruvnet/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "version": "dc97873b5cf6c708ed2b655fef43446a3bdf5e20",
      "committed_at": "2025-02-09T15:11:51Z",
      "change_status": {
        "total": 411,
        "additions": 411,
        "deletions": 0
      },
      "url": "https://api.github.com/gists/5eabfef63adddd272d756e42b131deb7/dc97873b5cf6c708ed2b655fef43446a3bdf5e20"
    }
  ],
  "truncated": false,
  "type": "gist",
  "commit": "dc97873b5cf6c708ed2b655fef43446a3bdf5e20",
  "lastUpdated": "2026-01-30T15:12:42+00:00"
}

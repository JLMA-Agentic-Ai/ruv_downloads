{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fine-tuning Large Language Models with QLoRA and FSDP\n",
    "\n",
    "Created by rUv, cause I could. This notebook demonstrates how to efficiently fine-tune large language models like Llama 2 70B using Quantized LoRA (QLoRA) and Fully Sharded Data Parallel (FSDP) on consumer hardware.\n",
    "\n",
    "Key concepts:\n",
    "- QLoRA enables fine-tuning large models by only training the LoRA adapter layers \n",
    "- FSDP allows sharded data parallel training to distribute the model across multiple GPUs\n",
    "- This makes it feasible to fine-tune 70B models on hardware like 2x 24GB GPUs\n",
    "\n",
    "We'll cover the installation, training options, and provide detailed examples.\n",
    "\n",
    "> Note: Treat this as an alpha/preview release. If you're not comfortable with testing and debugging models, we'd suggest holding off for a few months while the community more fully tests the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required dependencies. The following steps should work (tested on CUDA 11.7, 11.8 and 12.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the fsdp_qlora repo\n",
    "!git clone https://github.com/AnswerDotAI/fsdp_qlora\n",
    "\n",
    "# Install llama-recipes and fastcore\n",
    "!pip install llama-recipes fastcore --extra-index-url https://download.pytorch.org/whl/test/cu118\n",
    "\n",
    "# Install bitsandbytes\n",
    "!pip install bitsandbytes>=0.43.0\n",
    "\n",
    "# Login to Hugging Face to access Llama 2 model\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Optional libraries:\n",
    "- For HQQ quantization, follow the [HQQ installation instructions](https://github.com/mobiusml/hqq#installation). Make sure to also build the custom kernels.\n",
    "- For Weights and Biases logging: `!pip install wandb`\n",
    "\n",
    "PyTorch >= 2.2 is recommended to make use of the native flash-attention 2 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4", 
   "metadata": {},
   "source": [
    "## Fine-tuning Llama 2 70B on Dual 24GB GPUs\n",
    "\n",
    "Once installed, `cd` into the `fsdp_qlora` directory. Then run the following command to begin fine-tuning Llama 2 70B on the Alpaca dataset at a maximum sequence length of 2048 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "  --model_name meta-llama/Llama-2-70b-hf \\\n",
    "  --batch_size 2 \\\n",
    "  --context_length 2048 \\\n",
    "  --precision bf16 \\\n",
    "  --train_type qlora \\\n",
    "  --use_gradient_checkpointing true \\\n",
    "  --use_cpu_offload true \\\n",
    "  --dataset alpaca \\\n",
    "  --reentrant_checkpointing true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Training Options\n",
    "\n",
    "The training script supports various options for quantization, LoRA training, and mixed precision.\n",
    "\n",
    "### Quantization\n",
    "We support both HQQ and bitsandbytes for quantization. If using bitsandbytes, pass `--reentrant_checkpointing True` to avoid triggering a bug that results in high memory usage.\n",
    "\n",
    "### LoRA Training\n",
    "- `--train_type full`: Full model fine-tuning \n",
    "- `--train_type lora`: LoRA fine-tuning using HF PEFT library\n",
    "- `--train_type custom_lora`: LoRA fine-tuning using a custom LoRA module\n",
    "- `--train_type qlora`: 4-bit quantized LoRA using bitsandbytes Linear4bit layer with NF4 quantization and HF PEFT \n",
    "- `--train_type custom_qlora`: 4-bit quantized LoRA using bitsandbytes Linear4bit layer with NF4 quantization and custom LoRA module\n",
    "- `--train_type hqq_lora`: 4-bit quantized LoRA using HQQ library and custom LoRA module\n",
    "\n",
    "### Mixed Precision\n",
    "- `--precision bf16`: Pure bfloat16 training\n",
    "- `--precision fp32`: Pure float32 training  \n",
    "- `--precision mp_fp16_autocast`: Mixed float16 with autocast\n",
    "- `--precision mp_bf16_autocast`: Mixed bfloat16 with autocast\n",
    "- `--precision mp_bf16_buffers_autocast`: Bfloat16 params and float32 buffers with autocast (important for RoPE layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Here are a few detailed examples demonstrating different training configurations.\n",
    "\n",
    "### Full Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally set visible devices\n",
    "!export CUDA_VISIBLE_DEVICES=4,5\n",
    "\n",
    "!python train.py \\\n",
    "  --world_size 2 \\\n",
    "  --master_port 12356 \\\n",
    "  --model_name meta-llama/Llama-2-7b-hf \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --batch_size 8 \\\n",
    "  --context_length 512 \\\n",
    "  --precision bf16 \\\n",
    "  --train_type full \\\n",
    "  --use_gradient_checkpointing true \\\n",
    "  --use_cpu_offload false \\\n",
    "  --use_activation_cpu_offload false \\\n",
    "  --log_to wandb \\\n",
    "  --dataset alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### LoRA Fine-tuning with HF PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "  --model_name meta-llama/Llama-2-7b-hf \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --batch_size 8 \\\n",
    "  --context_length 512 \\\n",
    "  --precision bf16 \\\n",
    "  --train_type lora \\\n",
    "  --use_gradient_checkpointing true \\\n",
    "  --use_cpu_offload false \\\n",
    "  --use_activation_cpu_offload false \\\n",
    "  --log_to wandb \\\n",
    "  --dataset alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 4-bit QLoRA with Bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "  --model_name meta-llama/Llama-2-7b-hf \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --batch_size 8 \\\n",
    "  --context_length 512 \\\n",
    "  --precision bf16 \\\n",
    "  --train_type qlora \\\n",
    "  --use_gradient_checkpointing true \\\n",
    "  --use_cpu_offload false \\\n",
    "  --use_activation_cpu_offload false \\\n",
    "  --log_to wandb \\\n",
    "  --dataset alpaca \\\n",
    "  --reentrant_checkpointing true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 4-bit QLoRA with HQQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "  --model_name meta-llama/Llama-2-7b-hf \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --batch_size 8 \\\n",
    "  --context_length 512 \\\n",
    "  --precision bf16 \\\n",
    "  --train_type hqq_lora \\\n",
    "  --use_gradient_checkpointing true \\\n",
    "  --use_cpu_offload false \\\n",
    "  --use_activation_cpu_offload false \\\n",
    "  --log_to wandb \\\n",
    "  --dataset alpaca"
   ]
  },
  {
   "cell_type": "markdown",
      "id": "14",
   "metadata": {},
   "source": [
    "## SLURM Training\n",
    "\n",
    "Here's an example of how to run multi-node training with SLURM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15", 
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=fsdp_qlora\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=8\n",
    "#SBATCH --cpus-per-task=12\n",
    "#SBATCH --mem=480GB\n",
    "#SBATCH --gres=gpu:8\n",
    "#SBATCH --time=48:00:00\n",
    "#SBATCH --output=fsdp_qlora_%j.out\n",
    "\n",
    "module load anaconda3\n",
    "source activate fsdp_qlora\n",
    "\n",
    "srun python train.py \\\n",
    "  --model_name meta-llama/Llama-2-70b-hf \\\n",
    "  --batch_size 2 \\\n",
    "  --context_length 2048 \\\n",
    "  --precision bf16 \\\n",
    "  --train_type qlora \\\n",
    "  --use_gradient_checkpointing true \\\n",
    "  --use_cpu_offload true \\\n",
    "  --dataset alpaca \\\n",
    "  --reentrant_checkpointing true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This script requests 2 nodes with 8 GPUs each (16 total GPUs). It loads the fsdp_qlora conda environment and then launches the training script using `srun`.\n",
    "\n",
    "The training script will automatically detect it is running under SLURM and set up the distributed process group accordingly. \n",
    "\n",
    "Adjust the SLURM parameters like `--nodes`, `--ntasks-per-node`, `--gres` etc. based on your cluster configuration and model size.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered how to use QLoRA and FSDP to efficiently fine-tune large language models like Llama 2 70B on consumer hardware. \n",
    "\n",
    "The key takeaways are:\n",
    "\n",
    "- QLoRA enables fine-tuning large models by only training the LoRA adapter layers\n",
    "- FSDP allows sharded data parallel training to distribute the model across multiple GPUs \n",
    "- This makes it feasible to fine-tune 70B+ models on hardware like 2-4 GPUs\n",
    "- Use the appropriate training options for quantization (e.g. bitsandbytes, HQQ), LoRA variants, and mixed precision\n",
    "- Be aware of current limitations around model loading and saving\n",
    "\n",
    "We hope this notebook and example training script are helpful for your own large model fine-tuning experiments. Feel free to open issues or PRs in the [fsdp_qlora](https://github.com/AnswerDotAI/fsdp_qlora) repo with any questions or suggestions!\n",
    "\n",
    "Happy training! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
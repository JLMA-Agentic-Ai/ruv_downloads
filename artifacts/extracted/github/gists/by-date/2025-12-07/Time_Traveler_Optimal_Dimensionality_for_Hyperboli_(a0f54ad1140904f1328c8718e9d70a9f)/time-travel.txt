Optimal Dimensionality for Hyperbolic Vector Representations in HPC Simulations
Introduction

Hyperbolic vector embeddings have emerged as powerful representations for hierarchical and graph-structured data. Unlike Euclidean space, hyperbolic space has a constant negative curvature and an exponential volume growth, which allows it to embed tree-like structures with low distortion even in low dimensions
arxiv.org
. This makes hyperbolic embeddings attractive for high-performance simulations and machine learning tasks that involve hierarchical relationships (e.g. taxonomies or networks). However, choosing the optimal dimensionality for these embeddings is crucial to balance inference accuracy against computational speed, especially in real-time or microsecond-scale simulation environments. In such high-performance contexts (e.g. using a custom vector engine), each additional dimension incurs computational and memory overhead, so one must carefully trade off the richness of the representation against latency and numerical stability constraints.

Hyperbolic Embedding Models and Characteristics

Hyperbolic Embeddings: In an n-dimensional hyperbolic space (denoted ℍ^n), distances grow exponentially with radius
arxiv.org
. Intuitively, this provides “more room” to embed exponentially expanding structures like trees. For example, a hierarchy that would quickly crowd the edges of a 2D Euclidean space can be spread out without distortion in a 2D hyperbolic disk. Figure 1 illustrates this: a branching tree embedded in the Poincaré disk (a 2D hyperbolic model) can place many nodes at increasing radii without compressing distances, thanks to the disk’s negative curvature. Sibling nodes remain well-separated and parent-child distances stay uniform even near the boundary, avoiding the crowding observed in Euclidean embeddings
bjlkeng.io
bjlkeng.io
.

Figure 1: A hierarchical tree (binary branching) embedded in a 2D hyperbolic plane (Poincaré disk model). Distances from the center increase exponentially toward the boundary, allowing each level of the tree to occupy a “ring” of roughly equal thickness without overlapping
bjlkeng.io
. In a Euclidean plane, by contrast, nodes at deeper levels would crowd together at the outskirts (causing loss of distance resolution).

Poincaré vs. Lorentz Models: Two popular coordinate models for hyperbolic space are the Poincaré ball and the Lorentz (hyperboloid) model. Both are mathematically equivalent (related by isometry) but have different numerical properties. In the Poincaré ball model, points lie inside the unit ball in ℝ^n, and the distance between two vectors u and v is given by:

d_{\mathcal{H}}(u,v) \;=\; \cosh^{-1}\!\Big(1 \;+\; 2\,\frac{\|u-v\|^2}{(1-\|u\|^2)\,(1-\|v\|^2)}\Big)\,. \tag{1}

This formula involves Euclidean norms and an $\operatorname{arcosh}$, and it rapidly grows as points approach the boundary (‖u‖ or ‖v‖ → 1)
bjlkeng.io
. In contrast, the Lorentz model represents points on an n+1 dimensional hyperboloid defined in Minkowski space. A point is given by coordinates $(x_0, x_1,\dots,x_n)$ satisfying $-x_0^2 + x_1^2+\cdots+x_n^2 = -1$ (with $x_0>0$). The distance can be computed via the Minkowski inner product $\eta(x,y) = -x_0y_0 + \sum_{i=1}^n x_i y_i$ as:

d_{\mathcal{H}}(x,y) \;=\; \cosh^{-1}\!\big(-\,\eta(x,y)\big)\,. \tag{2}

Both models yield the same geometry but have different computational considerations. The Lorentz model’s distance formula is somewhat simpler (a single dot product plus $\cosh^{-1}$) and lacks the division by small numbers that appears in the Poincaré formula. This often translates to more stable gradient calculations in training and simpler distance computations in inference. However, the Lorentz coordinates include an extra dimension (for $x_0$) and can grow large in magnitude for points far from the origin, which has implications for numerical precision (discussed below).

Geometric Deep Learning in Hyperbolic Space: Many modern algorithms incorporate hyperbolic geometry to exploit these properties. Examples include Poincaré embeddings for hierarchical representations
arxiv.org
, hyperbolic graph neural networks, and knowledge graph embeddings. These methods often report that hyperbolic embeddings can represent complex hierarchies with far fewer dimensions than Euclidean embeddings for the same accuracy. By capturing hierarchical structure in the geometry (through negative curvature), a hyperbolic model can use each dimension efficiently to encode exponential growth patterns. This means that a relatively low-dimensional hyperbolic vector can encode a rich tree of relationships that would otherwise require a very high-dimensional Euclidean vector to achieve similar pairwise distance fidelity
arxiv.org
. The next section explores this dimensionality–accuracy trade-off in detail.

Dimensionality vs. Accuracy Trade-offs

A major advantage of hyperbolic representations is their ability to achieve high accuracy with low dimensionality. Researchers have observed that even 2–10 dimensions in hyperbolic space can often outperform hundreds of dimensions in Euclidean space for hierarchical data
arxiv.org
proceedings.mlr.press
. In one notable result, a two-dimensional hyperbolic embedding of the WordNet taxonomy achieved a mean average precision of 0.989, whereas a prior approach needed 200 dimensions in Euclidean space to reach only 0.87 MAP
arxiv.org
. This highlights that hyperbolic geometry can encode hierarchy implicitly, reducing the need for additional vector dimensions. In practical terms, a hyperbolic model finds it easier to place unrelated nodes far apart and related nodes close together without crowding, so each dimension provides significant discriminatory power.

Empirical studies consistently show diminishing returns beyond a certain number of hyperbolic dimensions. For example, Nickel & Kiela (2018) found that on a complex taxonomy (WordNet nouns), a 10-dimensional Lorentz-model embedding outperformed the best 200-dimensional embedding from their earlier Poincaré model
proceedings.mlr.press
. In fact, their Lorentz embeddings in just 2 dimensions already captured the hierarchy much more accurately (74% improvement in certain metrics) than Poincaré embeddings in 2D
proceedings.mlr.press
. Similarly, a summary of experiments from another study showed that with as few as 5 dimensions, Poincaré hyperbolic embeddings achieved massive improvements in reconstruction accuracy over Euclidean methods
bjlkeng.io
. In large-scale network embedding tasks, researchers often report using only about 5–10 dimensions in hyperbolic space, since the embedding dimension is typically small (e.g. 10) and yet yields lower distance distortion than a higher-dimensional Euclidean embedding
pure.tue.nl
. In other words, beyond a small number of dimensions, hyperbolic embeddings tend to saturate in performance: additional dimensions give only marginal gains because the negative curvature already provides an efficient encoding of similarity
arxiv.org
.

It is worth noting that the optimal dimensionality can depend on the data’s structure. Purely tree-structured data (like strict hierarchies) might reach peak accuracy at extremely low dimensions (2 or 5), whereas more complex graphs (with multiple latent factors or deviations from a perfect tree) may benefit from slightly higher dimensions (perhaps 10–20) before leveling off. Nonetheless, even in those cases, the required dimension is far lower than one would need in Euclidean space for comparable accuracy
arxiv.org
. This efficiency is a direct consequence of hyperbolic geometry’s ability to naturally represent hierarchical distances (exponential branching) in a linear or low-dimensional way
arxiv.org
.

Trade-off implications: From a simulation perspective, using the smallest sufficient dimension is desirable. Lower dimensionality means fewer coordinates to update or compute distances on, which translates to faster calculations. At the same time, too low a dimension might start to sacrifice fidelity if the data has more complex structure than the embedding can capture. The key is to find a balance where adding one more dimension yields negligible improvement in accuracy – that point can be considered an optimal dimension. Published benchmarks (as cited above) suggest that for many hierarchical datasets, this optimal point is often in the single digits or low tens of dimensions
bjlkeng.io
pure.tue.nl
. Selecting a dimension in that range typically “balances the scales” between accuracy and speed for hyperbolic embeddings.

Precision, Memory, and Numerical Stability Considerations

Choosing dimensionality in hyperbolic space is also tied to precision and numerical stability concerns, especially under tight computational budgets. Hyperbolic models can push coordinate values to extreme ranges when representing large or deep hierarchies, which strains finite precision arithmetic. There are two critical aspects: (1) the model (Poincaré vs Lorentz) and (2) the floating-point precision (single vs double) used in computation.

Poincaré model stability: In the Poincaré ball, points representing very distant nodes in a hierarchy gravitate exponentially close to the unit sphere boundary. This means coordinate differences can become very small (e.g. 1 − ‖u‖ is tiny), and distance calculations involve subtracting nearly equal quantities. As a result, to faithfully distinguish points near the boundary, one requires a large number of bits of precision
arxiv.org
. Sala et al. (2018) proved that embedding a tree with low distortion in the Poincaré model may demand high-precision arithmetic; using standard 64-bit doubles, there is essentially a maximum radius (a bit under the edge of the disk) beyond which points cannot be distinguished due to rounding
arxiv.org
arxiv.org
. In practical terms, this means a very deep hierarchy could cause 32-bit floats to produce severe rounding error or even NaNs, and even 64-bit floats can represent points only up to a certain depth (beyond which all points blur at the boundary)
arxiv.org
. Thus, the Poincaré model has a “hard” representation limit – with Float64, points approaching the boundary lose their accuracy entirely
arxiv.org
.

Lorentz model stability: The Lorentz model tends to have the opposite numerical issue. Distances increase when points move farther from the origin along the hyperboloid, meaning the time-like coordinate $x_0 = \cosh(\text{radial distance})$ can grow very large. Large values in $x_0$ or other coordinates can exceed the dynamic range or precision of floating-point representation. Yu & De Sa (2019) showed that using floats to represent Lorentz embeddings leads to huge errors once points are far out on the hyperboloid
arxiv.org
. Essentially, differences like $(x_0 + x_0') - x_0$ may lose precision if $x_0$ is on the order of $10^{16}$ and $x_0'$ is much smaller. Under 64-bit precision, the Lorentz model can faithfully represent points only within a certain radius as well, which in one analysis was even smaller (in hyperbolic distance) than that of the Poincaré model
arxiv.org
arxiv.org
. However, an important distinction is that the Lorentz model’s limit is a “soft” constraint: even if points are numerically inexact when extremely far out, one can often still compute with the large coordinates (e.g. for a while $x_0$ might overflow the guarantee of precise addition, but the coordinates are still finite and can be used in dot products)
arxiv.org
. This, combined with typically better optimization behavior, means Lorentz embeddings have been observed to be empirically more stable during training than Poincaré, despite the theoretical representation limits
arxiv.org
.

Precision vs. speed: Using double precision (64-bit) floating point is generally recommended for hyperbolic simulations if the hierarchy is deep, as it significantly extends the range of representable distances. Many hyperbolic embedding experiments are conducted in double precision specifically to avoid numerical issues
arxiv.org
. In a microsecond-scale simulation, however, double precision arithmetic might be slower (and doubles consume twice the memory of floats). If the hierarchical depth or required dynamic range is moderate, 32-bit floats might suffice and would improve speed and memory usage. For example, if all points stay within a radius that 32-bit can handle with acceptable rounding error, floats would cut memory bandwidth in half and likely fit better in vector registers, accelerating computation. The safe approach is to profile and determine if single precision causes any noticeable degradation in the distance calculations (e.g. by checking for NaNs or deviations in invariants). In many real-time applications with limited hierarchy levels, floats might perform well, but caution is warranted for any algorithm that might push values to extremes.

Memory and cache: Memory footprint grows linearly with dimensionality. An n-dimensional embedding vector in double precision uses 8n bytes. In an HPC context, large dimensions can quickly tax memory bandwidth – which is often a bottleneck in simulation – and also pressure caches. For instance, consider tracking 1 million entities with 16-dimensional vs. 128-dimensional hyperbolic vectors: the latter would consume 8× more memory and each simulation step would need to move 8× more data for the same computations. In a microsecond-scale loop, this difference is enormous. Keeping the dimensionality low (and using floats if possible) means more vectors or coordinates can reside in fast cache memory and fewer bytes need to be streamed from main memory per update. This directly improves throughput on modern HPC architectures. In fact, one study notes that using an embedding dimension around 10 allows distance computations in essentially constant time (since 10 values easily fit in vector registers or cache lines)
pure.tue.nl
. Meanwhile, extremely high dimensions would force multiple load/store operations and potentially cache misses, increasing latency per operation.

Vectorization: If using specialized vector hardware (e.g. SIMD units or a custom “ruvector” engine), the dimensionality should ideally align with hardware capabilities. Many vector processors operate on fixed-size blocks (e.g. 4, 8, or 16 numbers at once). If the dimension is a multiple of that width, distance calculations can be perfectly vectorized; if not, there might be under-utilization of some lanes or extra cycles to handle the remainder. For example, if an engine has a 16-wide vector unit, a 16-dimensional or 32-dimensional vector is optimal, whereas a 17-dimensional vector might incur an extra partial operation. This doesn’t usually override the primary consideration of embedding accuracy, but when fine-tuning for microsecond-level performance, it’s a factor. Choosing a dimension that is both minimal for accuracy and friendly to the hardware’s vector length can yield slight performance gains.

Recommendations for Selecting Dimensionality in Real-Time Simulation

Balancing speed and accuracy in a high-performance simulation context requires an informed choice of hyperbolic embedding dimensionality. Based on the above insights and published benchmarks, here are specific recommendations:

Start Low and Increase Gradually: Begin with the smallest dimension that conceptually fits the data (e.g. d = 2 or 5 for a simple tree hierarchy). Hyperbolic space is very expressive in low dimensions
arxiv.org
, so a minimal vector size often suffices. Increase d incrementally and monitor the improvement in simulation accuracy or distortion. There is usually a knee point where additional dimensions yield diminishing returns
arxiv.org
 – use that as the cutoff for optimal d.

Leverage Theoretical Trade-offs: Be aware that hyperbolic embeddings can trade off precision and dimensionality. If you find that a very low dimension causes slight distortion, consider whether a small increase in dimension can dramatically reduce error (as seen when going from 2D to 5D in some cases
bjlkeng.io
). Often a jump to around 5–10 dimensions captures most hierarchies with high fidelity, and going beyond that yields only minor gains. Aim to stay in the lowest dimensional regime that meets the accuracy criteria.

Choose the Right Model: For training or dynamic simulations that adjust the embeddings, the Lorentz model often provides more stable gradients and faster convergence in low dimensions
proceedings.mlr.press
arxiv.org
. Thus, if you are learning embeddings on the fly in your simulation, using the Lorentz model with Riemannian optimization can achieve a given accuracy in a smaller dimension or fewer iterations. For inference-only simulations (using fixed embeddings), either model is fine, but the Lorentz model’s distance computation (a single Minkowski dot product) may be slightly simpler to implement on vector hardware. Just keep an eye on the range of values: if your simulation might require extremely distant points, Poincaré coordinates have more headroom under double precision
arxiv.org
, whereas Lorentz might need careful handling to avoid overflow. In most practical cases (where radii are moderate), we recommend Lorentz for its robustness and efficiency, unless there is a specific reason the Poincaré model suits your application better.

Mind Precision Limits: If the simulation deals with a deep hierarchy or requires very high precision distance calculations, use 64-bit floating point for safety
arxiv.org
. Check if 32-bit floats are causing any anomalies (e.g., distances not obeying triangle inequality due to rounding, or points “sticking” on the boundary). For many real-time simulations with controlled conditions, 32-bit may be adequate and will double the speed of vector arithmetic and halve memory usage. But always validate – particularly if your hyperbolic vectors venture close to the boundary (Poincaré) or far from the origin (Lorentz), where rounding issues manifest. If necessary, impose a radius limit (clamping the maximum norm of vectors) to keep values within a safe numeric range
arxiv.org
, a practice used in some implementations to avoid instability.

Optimize Memory Access: Ensure that your data layout and dimension choice are optimized for the memory hierarchy. For example, storing vectors in a structure-of-arrays (SoA) format can allow contiguous memory access for each coordinate across many points, which vectorizes well. Align the dimensional size to cache line boundaries or the vector engine’s native width if possible. For instance, if using an engine with 256-bit registers (able to process four 64-bit doubles at once), a dimension that is a multiple of 4 will utilize this fully. While you should not drastically change the dimension purely for alignment, it’s something to consider if you’re deciding between, say, 7 or 8 dimensions for a marginal accuracy difference.

Benchmark in Context: Finally, use benchmarks or small-scale tests that mimic your simulation’s workload to evaluate the performance impact of dimensionality. Measure the per-step computation time and the accuracy metrics as you vary dimension. Publications have provided general guidance (e.g., hyperbolic embeddings often plateau by 10–20 dimensions or less
pure.tue.nl
proceedings.mlr.press
), but the optimal point for your specific simulation might differ. If, for example, 8 dimensions give you 99% of the accuracy of 16 dimensions, the speed gain from halving the vector size is likely worth the tiny accuracy drop in a microsecond-critical loop. Use such data to justify your choice.

In summary, hyperbolic vector representations allow one to use remarkably low dimensionality to capture complex hierarchical structures, which is a boon for high-performance, real-time simulations. By selecting the minimal effective dimension (often in the single-digits or teens) and using appropriate models and precision, you can achieve a sweet spot where the simulation runs at microsecond-scale speeds without compromising on the fidelity of distance or inference computations. Empirical studies and theory concur that hyperbolic embeddings offer excellent accuracy per dimension
arxiv.org
bjlkeng.io
, so the goal is to capitalize on that efficiency while respecting the computational limits of your hardware. Adhering to these guidelines will ensure that your simulation leverages the full power of hyperbolic geometry with a balanced approach to speed, precision, and accuracy.

References and Further Reading

Nickel, M. & Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. Advances in Neural Information Processing Systems. (Introduced the use of Poincaré ball model for embedding taxonomies; showed drastic reduction in required dimensions for hierarchical data.)

Nickel, M. & Kiela, D. (2018). Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry. ICML. (Demonstrated advantages of the Lorentz model and low-dimensional embeddings for large taxonomies, with comparisons to Poincaré.)

Sala, F. et al. (2018). Representation Tradeoffs for Hyperbolic Embeddings. ICML. (Provided theoretical bounds on precision vs. dimensionality in hyperbolic space; combinatorial constructions achieving low distortion with few dimensions at the cost of high precision.)
arxiv.org
arxiv.org

Mishne, G. et al. (2022). The Numerical Stability of Hyperbolic Representation Learning. ICML. (Analyzed numerical limits of Poincaré and Lorentz models under finite precision, and proposed an alternative parameterization to improve stability.)
arxiv.org
arxiv.org

Chami, I. et al. (2019). Hyperbolic Graph Convolutional Networks. NeurIPS. (Applied hyperbolic embeddings in deep learning for graph data; discusses selecting curvature and dimension in practice.)

Sarkar, R. (2011). Low Distortion Embeddings of Trees in Hyperbolic Plane. (Proved that trees can be embedded with arbitrarily low distortion in 2D hyperbolic space, motivating later machine learning applications
arxiv.org
.)